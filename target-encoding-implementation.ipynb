{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f754d51",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.011213,
     "end_time": "2021-08-25T21:38:15.323479",
     "exception": false,
     "start_time": "2021-08-25T21:38:15.312266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Target Encoding Implementation for 30 Days of ML Challenge, by Juan Torres\n",
    "\n",
    "#### Based on Abhishek Thakur's tutorials and notebooks:\n",
    "\n",
    "https://www.youtube.com/watch?v=2Yx2Y545yBk\n",
    "\n",
    "https://www.kaggle.com/abhishek/competition-part-3-target-encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422f52bf",
   "metadata": {
    "papermill": {
     "duration": 0.009749,
     "end_time": "2021-08-25T21:38:15.343604",
     "exception": false,
     "start_time": "2021-08-25T21:38:15.333855",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A quick word about our previous notebook. We switched from XGBoost to LightGBM but obtained worse results. We will see after parameter optimization which model works best, but for now we'll switch back to XGBoost.\n",
    "\n",
    "For this notebook, we will implement target encoding, which is a useful technique that averages the target value by category. A more detailed explanation of this technique can be found at https://maxhalford.github.io/blog/target-encoding/. This method requires that we change the way data is preprocessed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da89aa6",
   "metadata": {
    "papermill": {
     "duration": 0.009697,
     "end_time": "2021-08-25T21:38:15.363510",
     "exception": false,
     "start_time": "2021-08-25T21:38:15.353813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c538809c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T21:38:15.396085Z",
     "iopub.status.busy": "2021-08-25T21:38:15.395451Z",
     "iopub.status.idle": "2021-08-25T21:38:16.520893Z",
     "shell.execute_reply": "2021-08-25T21:38:16.520289Z",
     "shell.execute_reply.started": "2021-08-25T21:34:23.090127Z"
    },
    "papermill": {
     "duration": 1.147391,
     "end_time": "2021-08-25T21:38:16.521035",
     "exception": false,
     "start_time": "2021-08-25T21:38:15.373644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/train-folds-30-days-of-ml/train_folds.csv\n",
      "/kaggle/input/30-days-of-ml/sample_submission.csv\n",
      "/kaggle/input/30-days-of-ml/train.csv\n",
      "/kaggle/input/30-days-of-ml/test.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# For one-hot encoding categorical variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# For the construction of the pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# For training the XGBoost model\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# For the mean squared error needed to calculate our scores\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa7305",
   "metadata": {
    "papermill": {
     "duration": 0.011121,
     "end_time": "2021-08-25T21:38:16.543193",
     "exception": false,
     "start_time": "2021-08-25T21:38:16.532072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Loading and preparing data and pipeline construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c661f20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T21:38:16.568409Z",
     "iopub.status.busy": "2021-08-25T21:38:16.567821Z",
     "iopub.status.idle": "2021-08-25T21:38:20.249609Z",
     "shell.execute_reply": "2021-08-25T21:38:20.248686Z",
     "shell.execute_reply.started": "2021-08-25T21:34:24.159776Z"
    },
    "papermill": {
     "duration": 3.695942,
     "end_time": "2021-08-25T21:38:20.249803",
     "exception": false,
     "start_time": "2021-08-25T21:38:16.553861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the training and test data. \n",
    "X_full = pd.read_csv(\"../input/train-folds-30-days-of-ml/train_folds.csv\")\n",
    "X_test_full = pd.read_csv(\"../input/30-days-of-ml/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e80a90",
   "metadata": {
    "papermill": {
     "duration": 0.011799,
     "end_time": "2021-08-25T21:38:20.273883",
     "exception": false,
     "start_time": "2021-08-25T21:38:20.262084",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We have loaded the data, and now we will modify the data in order to do the target encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d65c00c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T21:38:20.341960Z",
     "iopub.status.busy": "2021-08-25T21:38:20.341067Z",
     "iopub.status.idle": "2021-08-25T21:38:26.869520Z",
     "shell.execute_reply": "2021-08-25T21:38:26.869045Z",
     "shell.execute_reply.started": "2021-08-25T21:34:28.188622Z"
    },
    "papermill": {
     "duration": 6.583996,
     "end_time": "2021-08-25T21:38:26.869675",
     "exception": false,
     "start_time": "2021-08-25T21:38:20.285679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We select all features except \"id\", \"target\" and \"kfold\", as these are not predictors of our target.\n",
    "useful_features = [c for c in X_full.columns if c not in (\"id\", \"target\", \"kfold\")]\n",
    "\n",
    "# Select numerical columns\n",
    "num_cols = [col for col in useful_features if 'cont' in col]\n",
    "\n",
    "# We select categorical columns. Note that we dropped the cardinality check.\n",
    "object_cols = [col for col in useful_features if 'cat' in col]\n",
    "\n",
    "# We build X_test out of X_test_full, but only selecting the useful features.\n",
    "X_test = X_test_full[useful_features]\n",
    "\n",
    "# Next up, we set up the for loop which will perform the target encoding:\n",
    "for col in object_cols: \n",
    "    temp_X_full = [] # We create a temporary list to store the dataframes.\n",
    "    temp_test_feature = None # We create a temporary feature for the test set.\n",
    "    \n",
    "    for fold in range(5): # We loop across all folds\n",
    "        X_train = X_full[X_full.kfold != fold].reset_index(drop=True) \n",
    "        X_valid = X_full[X_full.kfold == fold].reset_index(drop=True) \n",
    "        feat = X_train.groupby(col)[\"target\"].agg(\"mean\") # We group the columns by target, and then we get the mean value of the values in \"target\" column.\n",
    "        feat = feat.to_dict() # We convert the dataframe into a dictionary.\n",
    "        X_valid.loc[:, f\"tar_enc_{col}\"] = X_valid[col].map(feat) # We map the mean values to a new column in X_valid.\n",
    "        temp_X_full.append(X_valid) # We append X_valid to our temporary list.\n",
    "        \n",
    "        if temp_test_feature is None: # If we don't have a temp_test_feature...\n",
    "            temp_test_feature = X_test[col].map(feat) # ...we assign it this value.\n",
    "            \n",
    "        else: # If its not None, (for folds above 0)...\n",
    "            temp_test_feature = temp_test_feature + X_test[col].map(feat) # ...add to it the present value.\n",
    "            \n",
    "    temp_test_feature = temp_test_feature/5 # We divide by the number of folds to get the average.\n",
    "    X_test.loc[:, f\"tar_enc_{col}\"] = temp_test_feature # We assign the temp_test_feat value to a new column.\n",
    "    X_full = pd.concat(temp_X_full) # We build the new X_full dataframe with the new target encoding columns.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b573722",
   "metadata": {
    "papermill": {
     "duration": 0.010733,
     "end_time": "2021-08-25T21:38:26.891750",
     "exception": false,
     "start_time": "2021-08-25T21:38:26.881017",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Ok, let's take a peek at the reworked X_full:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "747d17c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T21:38:26.920698Z",
     "iopub.status.busy": "2021-08-25T21:38:26.920021Z",
     "iopub.status.idle": "2021-08-25T21:38:26.947002Z",
     "shell.execute_reply": "2021-08-25T21:38:26.947444Z",
     "shell.execute_reply.started": "2021-08-25T21:34:39.077737Z"
    },
    "papermill": {
     "duration": 0.045035,
     "end_time": "2021-08-25T21:38:26.947574",
     "exception": false,
     "start_time": "2021-08-25T21:38:26.902539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>...</th>\n",
       "      <th>tar_enc_cat0</th>\n",
       "      <th>tar_enc_cat1</th>\n",
       "      <th>tar_enc_cat2</th>\n",
       "      <th>tar_enc_cat3</th>\n",
       "      <th>tar_enc_cat4</th>\n",
       "      <th>tar_enc_cat5</th>\n",
       "      <th>tar_enc_cat6</th>\n",
       "      <th>tar_enc_cat7</th>\n",
       "      <th>tar_enc_cat8</th>\n",
       "      <th>tar_enc_cat9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>8.246824</td>\n",
       "      <td>8.204869</td>\n",
       "      <td>8.246016</td>\n",
       "      <td>8.277317</td>\n",
       "      <td>8.241689</td>\n",
       "      <td>8.251699</td>\n",
       "      <td>8.241846</td>\n",
       "      <td>8.300596</td>\n",
       "      <td>8.233186</td>\n",
       "      <td>8.241516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>8.240298</td>\n",
       "      <td>8.278027</td>\n",
       "      <td>8.246016</td>\n",
       "      <td>8.237701</td>\n",
       "      <td>8.241689</td>\n",
       "      <td>8.251699</td>\n",
       "      <td>8.241846</td>\n",
       "      <td>8.241403</td>\n",
       "      <td>8.233186</td>\n",
       "      <td>8.253157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>8.246824</td>\n",
       "      <td>8.278027</td>\n",
       "      <td>8.246016</td>\n",
       "      <td>8.277317</td>\n",
       "      <td>8.241689</td>\n",
       "      <td>8.251699</td>\n",
       "      <td>8.241846</td>\n",
       "      <td>8.241403</td>\n",
       "      <td>8.281416</td>\n",
       "      <td>8.260024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>8.240298</td>\n",
       "      <td>8.204869</td>\n",
       "      <td>8.246016</td>\n",
       "      <td>8.237701</td>\n",
       "      <td>8.241689</td>\n",
       "      <td>8.251699</td>\n",
       "      <td>8.241846</td>\n",
       "      <td>8.241403</td>\n",
       "      <td>8.252384</td>\n",
       "      <td>8.224612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>8.246824</td>\n",
       "      <td>8.278027</td>\n",
       "      <td>8.246016</td>\n",
       "      <td>8.237701</td>\n",
       "      <td>8.241689</td>\n",
       "      <td>8.251699</td>\n",
       "      <td>8.241846</td>\n",
       "      <td>8.241403</td>\n",
       "      <td>8.233186</td>\n",
       "      <td>8.233269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8  ... tar_enc_cat0  \\\n",
       "0   2    B    B    A    A    B    D    A    F    A  ...     8.246824   \n",
       "1   6    A    A    A    C    B    D    A    E    A  ...     8.240298   \n",
       "2   8    B    A    A    A    B    D    A    E    C  ...     8.246824   \n",
       "3  10    A    B    A    C    B    D    A    E    G  ...     8.240298   \n",
       "4  18    B    A    A    C    B    D    A    E    A  ...     8.246824   \n",
       "\n",
       "   tar_enc_cat1  tar_enc_cat2  tar_enc_cat3  tar_enc_cat4  tar_enc_cat5  \\\n",
       "0      8.204869      8.246016      8.277317      8.241689      8.251699   \n",
       "1      8.278027      8.246016      8.237701      8.241689      8.251699   \n",
       "2      8.278027      8.246016      8.277317      8.241689      8.251699   \n",
       "3      8.204869      8.246016      8.237701      8.241689      8.251699   \n",
       "4      8.278027      8.246016      8.237701      8.241689      8.251699   \n",
       "\n",
       "   tar_enc_cat6  tar_enc_cat7  tar_enc_cat8  tar_enc_cat9  \n",
       "0      8.241846      8.300596      8.233186      8.241516  \n",
       "1      8.241846      8.241403      8.233186      8.253157  \n",
       "2      8.241846      8.241403      8.281416      8.260024  \n",
       "3      8.241846      8.241403      8.252384      8.224612  \n",
       "4      8.241846      8.241403      8.233186      8.233269  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669d7820",
   "metadata": {
    "papermill": {
     "duration": 0.011669,
     "end_time": "2021-08-25T21:38:26.970974",
     "exception": false,
     "start_time": "2021-08-25T21:38:26.959305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As we can see, we produced a new column for each of the categorical columns in our data. Now we can use these new features for our model, but first we have to redefine the object columns and numerical columns, as our current definition would include the \"tar_enc_cat\" columns as object columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6a65c71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T21:38:27.000192Z",
     "iopub.status.busy": "2021-08-25T21:38:26.999088Z",
     "iopub.status.idle": "2021-08-25T21:38:27.081496Z",
     "shell.execute_reply": "2021-08-25T21:38:27.081005Z",
     "shell.execute_reply.started": "2021-08-25T21:36:34.376524Z"
    },
    "papermill": {
     "duration": 0.098625,
     "end_time": "2021-08-25T21:38:27.081632",
     "exception": false,
     "start_time": "2021-08-25T21:38:26.983007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We select all features except \"id\", \"target\" and \"kfold\", as these are not predictors of our target.\n",
    "useful_features = [c for c in X_full.columns if c not in (\"id\", \"target\", \"kfold\")]\n",
    "\n",
    "# Select numerical columns by data type, not by column name\n",
    "num_cols = [col for col in X_full[useful_features] if X_full[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "# We select categorical columns. Note that we dropped the cardinality check.\n",
    "object_cols = [col for col in useful_features if col.startswith(\"cat\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfe40314",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T21:38:27.109839Z",
     "iopub.status.busy": "2021-08-25T21:38:27.108571Z",
     "iopub.status.idle": "2021-08-25T21:38:27.110989Z",
     "shell.execute_reply": "2021-08-25T21:38:27.111356Z",
     "shell.execute_reply.started": "2021-08-25T21:36:38.307806Z"
    },
    "papermill": {
     "duration": 0.018759,
     "end_time": "2021-08-25T21:38:27.111473",
     "exception": false,
     "start_time": "2021-08-25T21:38:27.092714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data, we use a StandardScaler to apply standardization.\n",
    "numerical_transformer = preprocessing.StandardScaler()\n",
    "\n",
    "# Preprocessing for categorical data and one-hot encoding.\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, num_cols),('cat', categorical_transformer, object_cols)])\n",
    "\n",
    "# Define the model \n",
    "model = XGBRegressor(tree_method='gpu_hist', gpu_id=0, predictor=\"gpu_predictor\")\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fa3ad42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T21:38:27.141134Z",
     "iopub.status.busy": "2021-08-25T21:38:27.140259Z",
     "iopub.status.idle": "2021-08-25T21:38:47.565982Z",
     "shell.execute_reply": "2021-08-25T21:38:47.566525Z",
     "shell.execute_reply.started": "2021-08-25T21:36:53.064324Z"
    },
    "papermill": {
     "duration": 20.444579,
     "end_time": "2021-08-25T21:38:47.566741",
     "exception": false,
     "start_time": "2021-08-25T21:38:27.122162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7227030406324084\n",
      "1 0.7306875651326146\n",
      "2 0.7264380957570341\n",
      "3 0.7285041616809392\n",
      "4 0.7446395076435606\n",
      "0.7305944741693114 0.007499216522979915\n"
     ]
    }
   ],
   "source": [
    "# We set up a list to store the final predictions.\n",
    "final_predictions = []\n",
    "\n",
    "# We set up a list for storing the mean non squared error scores.\n",
    "scores = []\n",
    "\n",
    "# We set the loop to loop across all of the folds. Since we have 5 folds, the loop range will be range(5).\n",
    "for fold in range(5):\n",
    "    X_train = X_full[X_full.kfold != fold].reset_index(drop=True) # We set the training data to be all folds different from the current fold number in the loop. We also reset the indices.\n",
    "    X_valid = X_full[X_full.kfold == fold].reset_index(drop=True) # The validation data is the current fold number in the loop. We also reset the indices.\n",
    "    X_test_copy = X_test.copy() # We copy the original X_test to not alter or overwrite over it.\n",
    "    \n",
    "    y_train = X_train.target # We set the training target equal to the target in the training set. This has to be done every iteration (as the fold and the data changes).\n",
    "    y_valid = X_valid.target # We set the validation target equal to the target in the validation set. This has to be done every iteration (as the fold and the data changes).\n",
    "    \n",
    "    X_train = X_train[useful_features] # We set our training data to be the previously defined useful features of X_train.\n",
    "    X_valid = X_valid[useful_features] # We set our validation data to be the previously defined useful features of X_valid.\n",
    "    \n",
    "    # We activate the pipeline, which preprocesses the training data and fits the model (will take about 10 minutes to run)\n",
    "    my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    preds_valid = my_pipeline.predict(X_valid) # We instruct the pipeline to make predictions on X_valid.\n",
    "    preds_test = my_pipeline.predict(X_test) # We instruct the pipeline to make predictions on X_test.\n",
    "    final_predictions.append(preds_test) # We append each of the test predictions on to our final_predictions list.\n",
    "    rmse = mean_squared_error(y_valid, preds_valid, squared=False) # We store the mean non squared error in a variable.\n",
    "    print(fold, rmse) # Print the fold number, and the mean non squared error for each fold.\n",
    "    scores.append(rmse) # We append the rmse value to the scores list.\n",
    "    \n",
    "print(np.mean(scores), np.std(scores)) # Print the mean non square error average, and its standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f2b42c",
   "metadata": {
    "papermill": {
     "duration": 0.011914,
     "end_time": "2021-08-25T21:38:47.591801",
     "exception": false,
     "start_time": "2021-08-25T21:38:47.579887",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As always, let's stack these predictions and build the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "070b8f30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T21:38:47.620594Z",
     "iopub.status.busy": "2021-08-25T21:38:47.619744Z",
     "iopub.status.idle": "2021-08-25T21:38:47.627219Z",
     "shell.execute_reply": "2021-08-25T21:38:47.627656Z",
     "shell.execute_reply.started": "2021-08-25T21:37:26.779759Z"
    },
    "papermill": {
     "duration": 0.023936,
     "end_time": "2021-08-25T21:38:47.627782",
     "exception": false,
     "start_time": "2021-08-25T21:38:47.603846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = np.mean(np.column_stack(final_predictions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0749861c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T21:38:47.657404Z",
     "iopub.status.busy": "2021-08-25T21:38:47.656861Z",
     "iopub.status.idle": "2021-08-25T21:38:48.139925Z",
     "shell.execute_reply": "2021-08-25T21:38:48.139418Z",
     "shell.execute_reply.started": "2021-08-25T21:37:27.323690Z"
    },
    "papermill": {
     "duration": 0.500301,
     "end_time": "2021-08-25T21:38:48.140066",
     "exception": false,
     "start_time": "2021-08-25T21:38:47.639765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the predictions to a CSV file\n",
    "output = pd.DataFrame({'Id': X_test_full.id, 'target': predictions})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 40.132629,
   "end_time": "2021-08-25T21:38:48.760413",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-25T21:38:08.627784",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
