{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be040bf",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.022169,
     "end_time": "2021-08-25T16:41:34.021440",
     "exception": false,
     "start_time": "2021-08-25T16:41:33.999271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature engineering for 30 Days of ML Challenge, by Juan Torres\n",
    "\n",
    "#### Based on Abhishek Thakur's tutorials and notebooks:\n",
    "\n",
    "https://www.youtube.com/watch?v=tx3FoYdiFwA\n",
    "\n",
    "https://www.kaggle.com/abhishek/competition-part-2-feature-engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2084c904",
   "metadata": {
    "papermill": {
     "duration": 0.020564,
     "end_time": "2021-08-25T16:41:34.063264",
     "exception": false,
     "start_time": "2021-08-25T16:41:34.042700",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In our previous notebook we implemented k-folds into our previously built pipeline that used one-hot encoding with an XGBRegressor as a model, and achieved better results. From this point on we will follow Abhishek Thakur's videos for ideas on how to improve our model. Although we have spoken about parameter optimization in the past, first we will do some feature engineering on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed4f2e4",
   "metadata": {
    "papermill": {
     "duration": 0.020482,
     "end_time": "2021-08-25T16:41:34.104786",
     "exception": false,
     "start_time": "2021-08-25T16:41:34.084304",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8ff299",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T16:41:34.159749Z",
     "iopub.status.busy": "2021-08-25T16:41:34.159070Z",
     "iopub.status.idle": "2021-08-25T16:41:35.178077Z",
     "shell.execute_reply": "2021-08-25T16:41:35.178537Z",
     "shell.execute_reply.started": "2021-08-25T16:11:27.790594Z"
    },
    "papermill": {
     "duration": 1.052948,
     "end_time": "2021-08-25T16:41:35.178826",
     "exception": false,
     "start_time": "2021-08-25T16:41:34.125878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/train-folds-30-days-of-ml/train_folds.csv\n",
      "/kaggle/input/30-days-of-ml/sample_submission.csv\n",
      "/kaggle/input/30-days-of-ml/train.csv\n",
      "/kaggle/input/30-days-of-ml/test.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# For one-hot encoding categorical variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# from sklearn.model_selection import train_test_split We won't be needing this anymore!\n",
    "\n",
    "# For the construction of the pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# For training the XGBoost model\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920dc3f3",
   "metadata": {
    "papermill": {
     "duration": 0.021525,
     "end_time": "2021-08-25T16:41:35.222513",
     "exception": false,
     "start_time": "2021-08-25T16:41:35.200988",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Loading and preparing data and pipeline construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02d14702",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T16:41:35.269224Z",
     "iopub.status.busy": "2021-08-25T16:41:35.268599Z",
     "iopub.status.idle": "2021-08-25T16:41:38.868318Z",
     "shell.execute_reply": "2021-08-25T16:41:38.869123Z",
     "shell.execute_reply.started": "2021-08-25T16:11:28.815772Z"
    },
    "papermill": {
     "duration": 3.625696,
     "end_time": "2021-08-25T16:41:38.869282",
     "exception": false,
     "start_time": "2021-08-25T16:41:35.243586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the training and test data. \n",
    "X_full = pd.read_csv(\"../input/train-folds-30-days-of-ml/train_folds.csv\")\n",
    "X_test_full = pd.read_csv(\"../input/30-days-of-ml/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b694266",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T16:41:38.948539Z",
     "iopub.status.busy": "2021-08-25T16:41:38.948010Z",
     "iopub.status.idle": "2021-08-25T16:41:38.988267Z",
     "shell.execute_reply": "2021-08-25T16:41:38.989467Z",
     "shell.execute_reply.started": "2021-08-25T16:11:32.507754Z"
    },
    "papermill": {
     "duration": 0.085666,
     "end_time": "2021-08-25T16:41:38.989713",
     "exception": false,
     "start_time": "2021-08-25T16:41:38.904047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We select all features except \"id\", \"target\" and \"kfold\", as these are not predictors of our target.\n",
    "useful_features = [c for c in X_full.columns if c not in (\"id\", \"target\", \"kfold\")]\n",
    "\n",
    "# Select numerical columns\n",
    "num_cols = [col for col in useful_features if 'cont' in col]\n",
    "\n",
    "# We select categorical columns. Note that we dropped the cardinality check.\n",
    "object_cols = [col for col in useful_features if 'cat' in col]\n",
    "\n",
    "# We build X_test out of X_test_full, but only selecting the useful features.\n",
    "X_test = X_test_full[useful_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10625363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T16:41:39.048073Z",
     "iopub.status.busy": "2021-08-25T16:41:39.046068Z",
     "iopub.status.idle": "2021-08-25T16:41:39.048875Z",
     "shell.execute_reply": "2021-08-25T16:41:39.049343Z",
     "shell.execute_reply.started": "2021-08-25T16:11:32.549304Z"
    },
    "papermill": {
     "duration": 0.030483,
     "end_time": "2021-08-25T16:41:39.049473",
     "exception": false,
     "start_time": "2021-08-25T16:41:39.018990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data and one-hot encoding\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, num_cols),('cat', categorical_transformer, object_cols)])\n",
    "\n",
    "# Define the model \n",
    "model = XGBRegressor(tree_method='gpu_hist', gpu_id=0, predictor=\"gpu_predictor\") # In Abhishek's method random_state was altered with each fold (as random_state = fold), so we'll trade repeatability for some induced randomness.\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1945a490",
   "metadata": {
    "papermill": {
     "duration": 0.020821,
     "end_time": "2021-08-25T16:41:39.091719",
     "exception": false,
     "start_time": "2021-08-25T16:41:39.070898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will make some changes to our previous model:\n",
    "* We will make a new list called \"scores\", in which we will store the scores to obtain an average and a standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc5bc845",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T16:41:39.142405Z",
     "iopub.status.busy": "2021-08-25T16:41:39.141764Z",
     "iopub.status.idle": "2021-08-25T16:41:58.198653Z",
     "shell.execute_reply": "2021-08-25T16:41:58.197870Z",
     "shell.execute_reply.started": "2021-08-25T16:11:32.558657Z"
    },
    "papermill": {
     "duration": 19.085459,
     "end_time": "2021-08-25T16:41:58.198792",
     "exception": false,
     "start_time": "2021-08-25T16:41:39.113333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7233328877365391\n",
      "1 0.7303783657341386\n",
      "2 0.7263870188205617\n",
      "3 0.7254889329611524\n",
      "4 0.7201351391590578\n",
      "0.7251444688822899 0.0033891436272372347\n"
     ]
    }
   ],
   "source": [
    "# We set up a list to store the final predictions.\n",
    "final_predictions = []\n",
    "\n",
    "# We set up a list for storing the mean non squared error scores.\n",
    "scores = []\n",
    "\n",
    "# We set the loop to loop across all of the folds. Since we have 5 folds, the loop range will be range(5).\n",
    "for fold in range(5):\n",
    "    X_train = X_full[X_full.kfold != fold].reset_index(drop=True) # We set the training data to be all folds different from the current fold number in the loop. We also reset the indices.\n",
    "    X_valid = X_full[X_full.kfold == fold].reset_index(drop=True) # The validation data is the current fold number in the loop. We also reset the indices.\n",
    "    X_test_copy = X_test.copy() # We copy the original X_test to not alter or overwrite over it.\n",
    "    \n",
    "    y_train = X_train.target # We set the training target equal to the target in the training set. This has to be done every iteration (as the fold and the data changes).\n",
    "    y_valid = X_valid.target # We set the validation target equal to the target in the validation set. This has to be done every iteration (as the fold and the data changes).\n",
    "    \n",
    "    X_train = X_train[useful_features] # We set our training data to be the previously defined useful features of X_train.\n",
    "    X_valid = X_valid[useful_features] # We set our validation data to be the previously defined useful features of X_valid.\n",
    "    \n",
    "    # We activate the pipeline, which preprocesses the training data and fits the model (will take about 10 minutes to run)\n",
    "    my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    preds_valid = my_pipeline.predict(X_valid) # We instruct the pipeline to make predictions on X_valid.\n",
    "    preds_test = my_pipeline.predict(X_test) # We instruct the pipeline to make predictions on X_test.\n",
    "    final_predictions.append(preds_test) # We append each of the test predictions on to our final_predictions list.\n",
    "    rmse = mean_squared_error(y_valid, preds_valid, squared=False) # We store the mean non squared error in a variable.\n",
    "    print(fold, rmse) # Print the fold number, and the mean non squared error for each fold.\n",
    "    scores.append(rmse) # We append the rmse value to the scores list.\n",
    "    \n",
    "print(np.mean(scores), np.std(scores)) # Print the mean non square error average, and its standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d0c46",
   "metadata": {
    "papermill": {
     "duration": 0.022108,
     "end_time": "2021-08-25T16:41:58.243629",
     "exception": false,
     "start_time": "2021-08-25T16:41:58.221521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Okay, let's take another look at the data to see what we can see about the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "602b7ec9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T16:41:58.323973Z",
     "iopub.status.busy": "2021-08-25T16:41:58.323065Z",
     "iopub.status.idle": "2021-08-25T16:41:58.471220Z",
     "shell.execute_reply": "2021-08-25T16:41:58.471705Z",
     "shell.execute_reply.started": "2021-08-25T16:11:51.555309Z"
    },
    "papermill": {
     "duration": 0.205946,
     "end_time": "2021-08-25T16:41:58.471866",
     "exception": false,
     "start_time": "2021-08-25T16:41:58.265920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>target</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160266</td>\n",
       "      <td>0.310921</td>\n",
       "      <td>0.389470</td>\n",
       "      <td>0.267559</td>\n",
       "      <td>0.237281</td>\n",
       "      <td>0.377873</td>\n",
       "      <td>0.322401</td>\n",
       "      <td>0.869850</td>\n",
       "      <td>8.113634</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558922</td>\n",
       "      <td>0.516294</td>\n",
       "      <td>0.594928</td>\n",
       "      <td>0.341439</td>\n",
       "      <td>0.906013</td>\n",
       "      <td>0.921701</td>\n",
       "      <td>0.261975</td>\n",
       "      <td>0.465083</td>\n",
       "      <td>8.481233</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375348</td>\n",
       "      <td>0.902567</td>\n",
       "      <td>0.555205</td>\n",
       "      <td>0.843531</td>\n",
       "      <td>0.748809</td>\n",
       "      <td>0.620126</td>\n",
       "      <td>0.541474</td>\n",
       "      <td>0.763846</td>\n",
       "      <td>8.364351</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239061</td>\n",
       "      <td>0.732948</td>\n",
       "      <td>0.679618</td>\n",
       "      <td>0.574844</td>\n",
       "      <td>0.346010</td>\n",
       "      <td>0.714610</td>\n",
       "      <td>0.540150</td>\n",
       "      <td>0.280682</td>\n",
       "      <td>8.049253</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.420667</td>\n",
       "      <td>0.648182</td>\n",
       "      <td>0.684501</td>\n",
       "      <td>0.956692</td>\n",
       "      <td>1.000773</td>\n",
       "      <td>0.776742</td>\n",
       "      <td>0.625849</td>\n",
       "      <td>0.250823</td>\n",
       "      <td>7.972260</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>499993</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450538</td>\n",
       "      <td>0.934360</td>\n",
       "      <td>1.005077</td>\n",
       "      <td>0.853726</td>\n",
       "      <td>0.422541</td>\n",
       "      <td>1.063463</td>\n",
       "      <td>0.697685</td>\n",
       "      <td>0.506404</td>\n",
       "      <td>7.945605</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>499996</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>0.508502</td>\n",
       "      <td>0.358247</td>\n",
       "      <td>0.257825</td>\n",
       "      <td>0.433525</td>\n",
       "      <td>0.301015</td>\n",
       "      <td>0.268447</td>\n",
       "      <td>0.577055</td>\n",
       "      <td>0.823611</td>\n",
       "      <td>7.326118</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>499997</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372425</td>\n",
       "      <td>0.364936</td>\n",
       "      <td>0.383224</td>\n",
       "      <td>0.551825</td>\n",
       "      <td>0.661007</td>\n",
       "      <td>0.629606</td>\n",
       "      <td>0.714139</td>\n",
       "      <td>0.245732</td>\n",
       "      <td>8.706755</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>499998</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424243</td>\n",
       "      <td>0.382028</td>\n",
       "      <td>0.468819</td>\n",
       "      <td>0.351036</td>\n",
       "      <td>0.288768</td>\n",
       "      <td>0.611169</td>\n",
       "      <td>0.380254</td>\n",
       "      <td>0.332030</td>\n",
       "      <td>7.229569</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>499999</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328669</td>\n",
       "      <td>0.789165</td>\n",
       "      <td>0.960406</td>\n",
       "      <td>0.776019</td>\n",
       "      <td>0.734707</td>\n",
       "      <td>0.484392</td>\n",
       "      <td>0.639754</td>\n",
       "      <td>0.689317</td>\n",
       "      <td>8.631146</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8  ...     cont6  \\\n",
       "0            1    B    B    B    C    B    B    A    E    C  ...  0.160266   \n",
       "1            2    B    B    A    A    B    D    A    F    A  ...  0.558922   \n",
       "2            3    A    A    A    C    B    D    A    D    A  ...  0.375348   \n",
       "3            4    B    B    A    C    B    D    A    E    C  ...  0.239061   \n",
       "4            6    A    A    A    C    B    D    A    E    A  ...  0.420667   \n",
       "...        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...       ...   \n",
       "299995  499993    B    B    A    A    B    D    A    E    A  ...  0.450538   \n",
       "299996  499996    A    B    A    C    B    B    A    E    E  ...  0.508502   \n",
       "299997  499997    B    B    A    C    B    C    A    E    G  ...  0.372425   \n",
       "299998  499998    A    B    A    C    B    B    A    E    E  ...  0.424243   \n",
       "299999  499999    A    A    A    C    A    D    A    E    A  ...  0.328669   \n",
       "\n",
       "           cont7     cont8     cont9    cont10    cont11    cont12    cont13  \\\n",
       "0       0.310921  0.389470  0.267559  0.237281  0.377873  0.322401  0.869850   \n",
       "1       0.516294  0.594928  0.341439  0.906013  0.921701  0.261975  0.465083   \n",
       "2       0.902567  0.555205  0.843531  0.748809  0.620126  0.541474  0.763846   \n",
       "3       0.732948  0.679618  0.574844  0.346010  0.714610  0.540150  0.280682   \n",
       "4       0.648182  0.684501  0.956692  1.000773  0.776742  0.625849  0.250823   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "299995  0.934360  1.005077  0.853726  0.422541  1.063463  0.697685  0.506404   \n",
       "299996  0.358247  0.257825  0.433525  0.301015  0.268447  0.577055  0.823611   \n",
       "299997  0.364936  0.383224  0.551825  0.661007  0.629606  0.714139  0.245732   \n",
       "299998  0.382028  0.468819  0.351036  0.288768  0.611169  0.380254  0.332030   \n",
       "299999  0.789165  0.960406  0.776019  0.734707  0.484392  0.639754  0.689317   \n",
       "\n",
       "          target  kfold  \n",
       "0       8.113634      4  \n",
       "1       8.481233      0  \n",
       "2       8.364351      4  \n",
       "3       8.049253      1  \n",
       "4       7.972260      0  \n",
       "...          ...    ...  \n",
       "299995  7.945605      4  \n",
       "299996  7.326118      4  \n",
       "299997  8.706755      3  \n",
       "299998  7.229569      4  \n",
       "299999  8.631146      3  \n",
       "\n",
       "[300000 rows x 27 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b386a4",
   "metadata": {
    "papermill": {
     "duration": 0.022754,
     "end_time": "2021-08-25T16:41:58.517942",
     "exception": false,
     "start_time": "2021-08-25T16:41:58.495188",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Not much huh? Since we don't have any idea as to what the data actually is, we can't infer much information just by eyeballing it, and feature engineering is usually done when we have some general ideas about the data, otherwise it will be just throwing spaghetti against the wall and seeing what sticks. Luckily, I like italian food. We have both numerical and categorical values, let's see what we can do about this data:\n",
    "* Standardization: Used for numerical values, we subtract the mean value from the feature and divide by the standard deviation.\n",
    "* Normalization: Used for numerical values.\n",
    "* Log transformation: Used for numerical values. We go to each column and replace every numerical column by the log of 1 plus x.\n",
    "* Polynomial Features: Used for numerical values. Sklearn function, generates a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39649639",
   "metadata": {
    "papermill": {
     "duration": 0.022847,
     "end_time": "2021-08-25T16:41:58.563755",
     "exception": false,
     "start_time": "2021-08-25T16:41:58.540908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Implementing Standardization\n",
    "\n",
    "For implementing standardization, we will change our pipeline to preprocess numerical data with a StandardScaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39c83821",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T16:41:58.615761Z",
     "iopub.status.busy": "2021-08-25T16:41:58.614990Z",
     "iopub.status.idle": "2021-08-25T16:41:58.617580Z",
     "shell.execute_reply": "2021-08-25T16:41:58.617182Z",
     "shell.execute_reply.started": "2021-08-25T16:11:51.742102Z"
    },
    "papermill": {
     "duration": 0.030793,
     "end_time": "2021-08-25T16:41:58.617703",
     "exception": false,
     "start_time": "2021-08-25T16:41:58.586910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data\n",
    "numerical_transformer_2 = preprocessing.StandardScaler() # We change the numerical transformer to use a StandardScaler\n",
    "\n",
    "# Preprocessing for categorical data and one-hot encoding\n",
    "categorical_transformer_2 = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor_2 = ColumnTransformer(transformers=[('num', numerical_transformer_2, num_cols),('cat', categorical_transformer_2, object_cols)])\n",
    "\n",
    "# Define the model \n",
    "model_2 = XGBRegressor(tree_method='gpu_hist', gpu_id=0, predictor=\"gpu_predictor\") # In Abhishek's method random_state was altered with each fold (as random_state = fold), so we'll trade repeatability for some induced randomness.\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline_2 = Pipeline(steps=[('preprocessor', preprocessor_2), ('model', model_2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf5c7de4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T16:41:58.671522Z",
     "iopub.status.busy": "2021-08-25T16:41:58.670627Z",
     "iopub.status.idle": "2021-08-25T16:42:16.688971Z",
     "shell.execute_reply": "2021-08-25T16:42:16.688523Z",
     "shell.execute_reply.started": "2021-08-25T16:11:51.751015Z"
    },
    "papermill": {
     "duration": 18.048481,
     "end_time": "2021-08-25T16:42:16.689095",
     "exception": false,
     "start_time": "2021-08-25T16:41:58.640614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7222957994558532\n",
      "1 0.7303877162646868\n",
      "2 0.7262597753893988\n",
      "3 0.7254887712952974\n",
      "4 0.7204276928254211\n",
      "0.7249719510461314 0.0034370969646553865\n"
     ]
    }
   ],
   "source": [
    "# We set up a list to store the final predictions.\n",
    "final_predictions_2 = []\n",
    "\n",
    "# We set up a list for storing the mean non squared error scores.\n",
    "scores_2 = []\n",
    "\n",
    "# We set the loop to loop across all of the folds. Since we have 5 folds, the loop range will be range(5).\n",
    "for fold in range(5):\n",
    "    X_train = X_full[X_full.kfold != fold].reset_index(drop=True) # We set the training data to be all folds different from the current fold number in the loop. We also reset the indices.\n",
    "    X_valid = X_full[X_full.kfold == fold].reset_index(drop=True) # The validation data is the current fold number in the loop. We also reset the indices.\n",
    "    X_test_copy = X_test.copy() # We copy the original X_test to not alter or overwrite over it.\n",
    "    \n",
    "    y_train = X_train.target # We set the training target equal to the target in the training set. This has to be done every iteration (as the fold and the data changes).\n",
    "    y_valid = X_valid.target # We set the validation target equal to the target in the validation set. This has to be done every iteration (as the fold and the data changes).\n",
    "    \n",
    "    X_train = X_train[useful_features] # We set our training data to be the previously defined useful features of X_train.\n",
    "    X_valid = X_valid[useful_features] # We set our validation data to be the previously defined useful features of X_valid.\n",
    "    \n",
    "    # We activate the pipeline, which preprocesses the training data and fits the model (will take about 10 minutes to run)\n",
    "    my_pipeline_2.fit(X_train, y_train)\n",
    "\n",
    "    preds_valid_2 = my_pipeline_2.predict(X_valid) # We instruct the pipeline to make predictions on X_valid.\n",
    "    preds_test_2 = my_pipeline_2.predict(X_test) # We instruct the pipeline to make predictions on X_test.\n",
    "    final_predictions_2.append(preds_test_2) # We append each of the test predictions on to our final_predictions list.\n",
    "    rmse_2 = mean_squared_error(y_valid, preds_valid_2, squared=False) # We store the mean non squared error in a variable.\n",
    "    print(fold, rmse_2) # Print the fold number, and the mean non squared error for each fold.\n",
    "    scores_2.append(rmse_2) # We append the rmse value to the scores list.\n",
    "    \n",
    "print(np.mean(scores_2), np.std(scores_2)) # Print the mean non square error average, and its standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b15c3",
   "metadata": {
    "papermill": {
     "duration": 0.024067,
     "end_time": "2021-08-25T16:42:16.737915",
     "exception": false,
     "start_time": "2021-08-25T16:42:16.713848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We got an improvement in most of our folds! So, even if we know nothing about our data, it seems trying out standardization is worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fec5f5b",
   "metadata": {
    "papermill": {
     "duration": 0.024082,
     "end_time": "2021-08-25T16:42:16.786232",
     "exception": false,
     "start_time": "2021-08-25T16:42:16.762150",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Implementing Normalization\n",
    "\n",
    "For implementing normalization, we will change our pipeline to preprocess numerical data with a Normalizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8964d0dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T16:42:16.841375Z",
     "iopub.status.busy": "2021-08-25T16:42:16.840098Z",
     "iopub.status.idle": "2021-08-25T16:42:16.842510Z",
     "shell.execute_reply": "2021-08-25T16:42:16.842938Z",
     "shell.execute_reply.started": "2021-08-25T16:12:09.913933Z"
    },
    "papermill": {
     "duration": 0.032507,
     "end_time": "2021-08-25T16:42:16.843060",
     "exception": false,
     "start_time": "2021-08-25T16:42:16.810553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data\n",
    "numerical_transformer_3 = preprocessing.Normalizer() # We change the numerical transformer to use a normalizer\n",
    "\n",
    "# Preprocessing for categorical data and one-hot encoding\n",
    "categorical_transformer_3 = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor_3 = ColumnTransformer(transformers=[('num', numerical_transformer_3, num_cols),('cat', categorical_transformer_3, object_cols)])\n",
    "\n",
    "# Define the model \n",
    "model_3 = XGBRegressor(tree_method='gpu_hist', gpu_id=0, predictor=\"gpu_predictor\") # In Abhishek's method random_state was altered with each fold (as random_state = fold), so we'll trade repeatability for some induced randomness.\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline_3 = Pipeline(steps=[('preprocessor', preprocessor_3), ('model', model_3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "759516b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T16:42:16.899295Z",
     "iopub.status.busy": "2021-08-25T16:42:16.898502Z",
     "iopub.status.idle": "2021-08-25T16:42:34.994149Z",
     "shell.execute_reply": "2021-08-25T16:42:34.993466Z",
     "shell.execute_reply.started": "2021-08-25T16:12:09.923972Z"
    },
    "papermill": {
     "duration": 18.127245,
     "end_time": "2021-08-25T16:42:34.994303",
     "exception": false,
     "start_time": "2021-08-25T16:42:16.867058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7374587462192872\n",
      "1 0.7451464464387808\n",
      "2 0.740816174993611\n",
      "3 0.7390018751824272\n",
      "4 0.7354334328295111\n",
      "0.7395713351327234 0.003302757768207594\n"
     ]
    }
   ],
   "source": [
    "# We set up a list to store the final predictions.\n",
    "final_predictions_3 = []\n",
    "\n",
    "# We set up a list for storing the mean non squared error scores.\n",
    "scores_3 = []\n",
    "\n",
    "# We set the loop to loop across all of the folds. Since we have 5 folds, the loop range will be range(5).\n",
    "for fold in range(5):\n",
    "    X_train = X_full[X_full.kfold != fold].reset_index(drop=True) # We set the training data to be all folds different from the current fold number in the loop. We also reset the indices.\n",
    "    X_valid = X_full[X_full.kfold == fold].reset_index(drop=True) # The validation data is the current fold number in the loop. We also reset the indices.\n",
    "    X_test_copy = X_test.copy() # We copy the original X_test to not alter or overwrite over it.\n",
    "    \n",
    "    y_train = X_train.target # We set the training target equal to the target in the training set. This has to be done every iteration (as the fold and the data changes).\n",
    "    y_valid = X_valid.target # We set the validation target equal to the target in the validation set. This has to be done every iteration (as the fold and the data changes).\n",
    "    \n",
    "    X_train = X_train[useful_features] # We set our training data to be the previously defined useful features of X_train.\n",
    "    X_valid = X_valid[useful_features] # We set our validation data to be the previously defined useful features of X_valid.\n",
    "    \n",
    "    # We activate the pipeline, which preprocesses the training data and fits the model (will take about 10 minutes to run)\n",
    "    my_pipeline_3.fit(X_train, y_train)\n",
    "\n",
    "    preds_valid_3 = my_pipeline_3.predict(X_valid) # We instruct the pipeline to make predictions on X_valid.\n",
    "    preds_test_3 = my_pipeline_3.predict(X_test) # We instruct the pipeline to make predictions on X_test.\n",
    "    final_predictions_3.append(preds_test_3) # We append each of the test predictions on to our final_predictions list.\n",
    "    rmse_3 = mean_squared_error(y_valid, preds_valid_3, squared=False) # We store the mean non squared error in a variable.\n",
    "    print(fold, rmse_3) # Print the fold number, and the mean non squared error for each fold.\n",
    "    scores_3.append(rmse_3) # We append the rmse value to the scores list.\n",
    "    \n",
    "print(np.mean(scores_3), np.std(scores_3)) # Print the mean non square error average, and its standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92920c03",
   "metadata": {
    "papermill": {
     "duration": 0.025521,
     "end_time": "2021-08-25T16:42:35.046006",
     "exception": false,
     "start_time": "2021-08-25T16:42:35.020485",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We got worse results, bummer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8235d46a",
   "metadata": {
    "papermill": {
     "duration": 0.025467,
     "end_time": "2021-08-25T16:42:35.097114",
     "exception": false,
     "start_time": "2021-08-25T16:42:35.071647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Implementing Log Transformation\n",
    "\n",
    "For implementing Log Transformation, we will have to modify the X_full dataframe and the X_test dataframe. We will use the first pipeline to see the effects of log transformation without any standardization or normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5f3c05d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T16:42:35.157680Z",
     "iopub.status.busy": "2021-08-25T16:42:35.156403Z",
     "iopub.status.idle": "2021-08-25T16:42:53.116180Z",
     "shell.execute_reply": "2021-08-25T16:42:53.117111Z",
     "shell.execute_reply.started": "2021-08-25T16:12:27.677303Z"
    },
    "papermill": {
     "duration": 17.994377,
     "end_time": "2021-08-25T16:42:53.117323",
     "exception": false,
     "start_time": "2021-08-25T16:42:35.122946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.722199574384611\n",
      "1 0.7303888797180302\n",
      "2 0.7263925761392298\n",
      "3 0.7254851008898957\n",
      "4 0.7201301016597935\n",
      "0.7249192465583121 0.0035423241995017275\n"
     ]
    }
   ],
   "source": [
    "# We set up a list to store the final predictions.\n",
    "final_predictions_4 = []\n",
    "\n",
    "# We set up a list for storing the mean non squared error scores.\n",
    "scores_4 = []\n",
    "\n",
    "# We set up copies of the training and test data to not overwrite it.\n",
    "X_full_copy = X_full.copy()\n",
    "X_test_copy = X_test.copy()\n",
    "\n",
    "# We perform log transformation on our training and test sets\n",
    "for col in num_cols:\n",
    "    X_full_copy[col] = np.log1p(X_full_copy[col])\n",
    "    X_test_copy[col] = np.log1p(X_test_copy[col])\n",
    "\n",
    "# We set the loop to loop across all of the folds. Since we have 5 folds, the loop range will be range(5).\n",
    "for fold in range(5):\n",
    "    X_train = X_full_copy[X_full_copy.kfold != fold].reset_index(drop=True) # We set the training data to be all folds different from the current fold number in the loop. We also reset the indices.\n",
    "    X_valid = X_full_copy[X_full_copy.kfold == fold].reset_index(drop=True) # The validation data is the current fold number in the loop. We also reset the indices.\n",
    "    X_test_copy = X_test.copy() # We copy the original X_test to not alter or overwrite over it.\n",
    "    \n",
    "    y_train = X_train.target # We set the training target equal to the target in the training set. This has to be done every iteration (as the fold and the data changes).\n",
    "    y_valid = X_valid.target # We set the validation target equal to the target in the validation set. This has to be done every iteration (as the fold and the data changes).\n",
    "    \n",
    "    X_train = X_train[useful_features] # We set our training data to be the previously defined useful features of X_train.\n",
    "    X_valid = X_valid[useful_features] # We set our validation data to be the previously defined useful features of X_valid.\n",
    "    \n",
    "    # We activate the pipeline, which preprocesses the training data and fits the model (will take about 10 minutes to run)\n",
    "    my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    preds_valid_4 = my_pipeline.predict(X_valid) # We instruct the pipeline to make predictions on X_valid.\n",
    "    preds_test_4 = my_pipeline.predict(X_test) # We instruct the pipeline to make predictions on X_test.\n",
    "    final_predictions_4.append(preds_test_4) # We append each of the test predictions on to our final_predictions list.\n",
    "    rmse_4 = mean_squared_error(y_valid, preds_valid_4, squared=False) # We store the mean non squared error in a variable.\n",
    "    print(fold, rmse_4) # Print the fold number, and the mean non squared error for each fold.\n",
    "    scores_4.append(rmse_4) # We append the rmse value to the scores list.\n",
    "    \n",
    "print(np.mean(scores_4), np.std(scores_4)) # Print the mean non square error average, and its standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eca450",
   "metadata": {
    "papermill": {
     "duration": 0.026812,
     "end_time": "2021-08-25T16:42:53.172105",
     "exception": false,
     "start_time": "2021-08-25T16:42:53.145293",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We also got a slight improvement using this method. Next up, we can mix different types of feature engineering to see if we can achieve even better results! Let's do a to-do list of feature engineering methods:\n",
    "* Log Transform + Standardization\n",
    "* Standardization of one-hot encoding & numerical values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b082ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-24T21:56:22.059333Z",
     "iopub.status.busy": "2021-08-24T21:56:22.058969Z",
     "iopub.status.idle": "2021-08-24T21:56:22.063638Z",
     "shell.execute_reply": "2021-08-24T21:56:22.062378Z",
     "shell.execute_reply.started": "2021-08-24T21:56:22.059301Z"
    },
    "papermill": {
     "duration": 0.026672,
     "end_time": "2021-08-25T16:42:53.225779",
     "exception": false,
     "start_time": "2021-08-25T16:42:53.199107",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Implementing Log Transformation + Standardization\n",
    "\n",
    "Based off this question on stackexchange (https://stats.stackexchange.com/questions/402470/how-can-i-use-scaling-and-log-transforming-together), we should first apply the log transformation, and then standardize the data. We already have a pipeline working with my_pipeline_2, so we just need to apply log transformation and then apply that pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9931ab4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T16:42:53.289095Z",
     "iopub.status.busy": "2021-08-25T16:42:53.287851Z",
     "iopub.status.idle": "2021-08-25T16:43:11.752688Z",
     "shell.execute_reply": "2021-08-25T16:43:11.753068Z",
     "shell.execute_reply.started": "2021-08-25T16:12:46.321262Z"
    },
    "papermill": {
     "duration": 18.500096,
     "end_time": "2021-08-25T16:43:11.753218",
     "exception": false,
     "start_time": "2021-08-25T16:42:53.253122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7486244672376378\n",
      "1 0.7559871170628206\n",
      "2 0.7522413781353476\n",
      "3 0.750569989940971\n",
      "4 0.7473239151480965\n",
      "0.7509493735049747 0.003024605939871177\n"
     ]
    }
   ],
   "source": [
    "# We set up a list to store the final predictions.\n",
    "final_predictions_5 = []\n",
    "\n",
    "# We set up a list for storing the mean non squared error scores.\n",
    "scores_5 = []\n",
    "\n",
    "# We set up copies of the training and test data to not overwrite it.\n",
    "X_full_copy = X_full.copy()\n",
    "X_test_copy = X_test.copy()\n",
    "\n",
    "# We perform log transformation on our training and test sets\n",
    "for col in num_cols:\n",
    "    X_full_copy[col] = np.log1p(X_full_copy[col])\n",
    "    X_test_copy[col] = np.log1p(X_test_copy[col])\n",
    "\n",
    "# We set the loop to loop across all of the folds. Since we have 5 folds, the loop range will be range(5).\n",
    "for fold in range(5):\n",
    "    X_train = X_full_copy[X_full_copy.kfold != fold].reset_index(drop=True) # We set the training data to be all folds different from the current fold number in the loop. We also reset the indices.\n",
    "    X_valid = X_full_copy[X_full_copy.kfold == fold].reset_index(drop=True) # The validation data is the current fold number in the loop. We also reset the indices.\n",
    "    X_test_copy = X_test.copy() # We copy the original X_test to not alter or overwrite over it.\n",
    "    \n",
    "    y_train = X_train.target # We set the training target equal to the target in the training set. This has to be done every iteration (as the fold and the data changes).\n",
    "    y_valid = X_valid.target # We set the validation target equal to the target in the validation set. This has to be done every iteration (as the fold and the data changes).\n",
    "    \n",
    "    X_train = X_train[useful_features] # We set our training data to be the previously defined useful features of X_train.\n",
    "    X_valid = X_valid[useful_features] # We set our validation data to be the previously defined useful features of X_valid.\n",
    "    \n",
    "    # We activate the pipeline, which preprocesses the training data and fits the model (will take about 10 minutes to run)\n",
    "    my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    preds_valid_5 = my_pipeline_2.predict(X_valid) # We instruct the pipeline to make predictions on X_valid.\n",
    "    preds_test_5 = my_pipeline_2.predict(X_test) # We instruct the pipeline to make predictions on X_test.\n",
    "    final_predictions_5.append(preds_test_5) # We append each of the test predictions on to our final_predictions list.\n",
    "    rmse_5 = mean_squared_error(y_valid, preds_valid_5, squared=False) # We store the mean non squared error in a variable.\n",
    "    print(fold, rmse_5) # Print the fold number, and the mean non squared error for each fold.\n",
    "    scores_5.append(rmse_5) # We append the rmse value to the scores list.\n",
    "    \n",
    "print(np.mean(scores_5), np.std(scores_5)) # Print the mean non square error average, and its standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb072830",
   "metadata": {
    "papermill": {
     "duration": 0.028009,
     "end_time": "2021-08-25T16:43:11.809775",
     "exception": false,
     "start_time": "2021-08-25T16:43:11.781766",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "That didn't bode so well either. Off to the next one:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c46885",
   "metadata": {
    "papermill": {
     "duration": 0.028,
     "end_time": "2021-08-25T16:43:11.865939",
     "exception": false,
     "start_time": "2021-08-25T16:43:11.837939",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Implementing Standardization of One-Hot Encoding & Numerical Values\n",
    "\n",
    "Sounds simple enough, apply one-hot encoding to the categorical values and then apply standardization on said one-hot values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52562031",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T16:43:11.929114Z",
     "iopub.status.busy": "2021-08-25T16:43:11.928291Z",
     "iopub.status.idle": "2021-08-25T16:43:11.931034Z",
     "shell.execute_reply": "2021-08-25T16:43:11.930607Z",
     "shell.execute_reply.started": "2021-08-25T16:13:04.484090Z"
    },
    "papermill": {
     "duration": 0.03701,
     "end_time": "2021-08-25T16:43:11.931154",
     "exception": false,
     "start_time": "2021-08-25T16:43:11.894144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data\n",
    "numerical_transformer_4 = preprocessing.StandardScaler() # We change the numerical transformer to use a normalizer\n",
    "\n",
    "# Preprocessing for categorical data and one-hot encoding\n",
    "categorical_transformer_4 = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore')), ('scaler', preprocessing.StandardScaler(with_mean=False))])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor_4 = ColumnTransformer(transformers=[('num', numerical_transformer_4, num_cols),('cat', categorical_transformer_4, object_cols)])\n",
    "\n",
    "# Define the model \n",
    "model_4 = XGBRegressor(tree_method='gpu_hist', gpu_id=0, predictor=\"gpu_predictor\") # In Abhishek's method random_state was altered with each fold (as random_state = fold), so we'll trade repeatability for some induced randomness.\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline_4 = Pipeline(steps=[('preprocessor', preprocessor_4), ('model', model_4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cec26858",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T16:43:11.996693Z",
     "iopub.status.busy": "2021-08-25T16:43:11.995810Z",
     "iopub.status.idle": "2021-08-25T16:43:30.728383Z",
     "shell.execute_reply": "2021-08-25T16:43:30.727520Z",
     "shell.execute_reply.started": "2021-08-25T16:13:04.493857Z"
    },
    "papermill": {
     "duration": 18.768959,
     "end_time": "2021-08-25T16:43:30.728542",
     "exception": false,
     "start_time": "2021-08-25T16:43:11.959583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7222957994558532\n",
      "1 0.7303877162646868\n",
      "2 0.7262597753893988\n",
      "3 0.7254887712952974\n",
      "4 0.7204276928254211\n",
      "0.7249719510461314 0.0034370969646553865\n"
     ]
    }
   ],
   "source": [
    "# We set up a list to store the final predictions.\n",
    "final_predictions_6 = []\n",
    "\n",
    "# We set up a list for storing the mean non squared error scores.\n",
    "scores_6 = []\n",
    "\n",
    "# We set the loop to loop across all of the folds. Since we have 5 folds, the loop range will be range(5).\n",
    "for fold in range(5):\n",
    "    X_train = X_full[X_full.kfold != fold].reset_index(drop=True) # We set the training data to be all folds different from the current fold number in the loop. We also reset the indices.\n",
    "    X_valid = X_full[X_full.kfold == fold].reset_index(drop=True) # The validation data is the current fold number in the loop. We also reset the indices.\n",
    "    X_test_copy = X_test.copy() # We copy the original X_test to not alter or overwrite over it.\n",
    "    \n",
    "    y_train = X_train.target # We set the training target equal to the target in the training set. This has to be done every iteration (as the fold and the data changes).\n",
    "    y_valid = X_valid.target # We set the validation target equal to the target in the validation set. This has to be done every iteration (as the fold and the data changes).\n",
    "    \n",
    "    X_train = X_train[useful_features] # We set our training data to be the previously defined useful features of X_train.\n",
    "    X_valid = X_valid[useful_features] # We set our validation data to be the previously defined useful features of X_valid.\n",
    "    \n",
    "    # We activate the pipeline, which preprocesses the training data and fits the model (will take about 10 minutes to run)\n",
    "    my_pipeline_4.fit(X_train, y_train)\n",
    "\n",
    "    preds_valid_6 = my_pipeline_4.predict(X_valid) # We instruct the pipeline to make predictions on X_valid.\n",
    "    preds_test_6 = my_pipeline_4.predict(X_test) # We instruct the pipeline to make predictions on X_test.\n",
    "    final_predictions_6.append(preds_test_6) # We append each of the test predictions on to our final_predictions list.\n",
    "    rmse_6 = mean_squared_error(y_valid, preds_valid_6, squared=False) # We store the mean non squared error in a variable.\n",
    "    print(fold, rmse_6) # Print the fold number, and the mean non squared error for each fold.\n",
    "    scores_6.append(rmse_6) # We append the rmse value to the scores list.\n",
    "    \n",
    "print(np.mean(scores_6), np.std(scores_6)) # Print the mean non square error average, and its standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9440b4a2",
   "metadata": {
    "papermill": {
     "duration": 0.02936,
     "end_time": "2021-08-25T16:43:30.788555",
     "exception": false,
     "start_time": "2021-08-25T16:43:30.759195",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Looks like we got better results, let's compare each method we have tried up to this point and select the one that gives us the smallest mean error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7506769c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T16:43:30.856098Z",
     "iopub.status.busy": "2021-08-25T16:43:30.855379Z",
     "iopub.status.idle": "2021-08-25T16:43:30.860405Z",
     "shell.execute_reply": "2021-08-25T16:43:30.859831Z",
     "shell.execute_reply.started": "2021-08-25T16:23:21.124059Z"
    },
    "papermill": {
     "duration": 0.042192,
     "end_time": "2021-08-25T16:43:30.860555",
     "exception": false,
     "start_time": "2021-08-25T16:43:30.818363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method with the lowest mean error is: Log Transformation\n"
     ]
    }
   ],
   "source": [
    "scores_dict = dict()\n",
    "scores_dict['No Changes'] = (np.mean(scores), np.std(scores))\n",
    "scores_dict['Standardization'] = (np.mean(scores_2), np.std(scores_2))\n",
    "scores_dict['Normalization'] = (np.mean(scores_3), np.std(scores_3))\n",
    "scores_dict['Log Transformation'] = (np.mean(scores_4), np.std(scores_4))\n",
    "scores_dict['Log Transformation + Standardization'] = (np.mean(scores_5), np.std(scores_5))\n",
    "scores_dict['Standardization of One-Hot Encoding & Numerical Values'] = (np.mean(scores_6), np.std(scores_6))\n",
    "print(\"The method with the lowest mean error is: \" + str(min(scores_dict, key=scores_dict.get)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfa530e",
   "metadata": {
    "papermill": {
     "duration": 0.030108,
     "end_time": "2021-08-25T16:43:30.921008",
     "exception": false,
     "start_time": "2021-08-25T16:43:30.890900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Looks like for this specific case Log Transformation is the way to go. Although, one interesting thing is that Log Transformation also has the highest standard deviation of all methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4857748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T16:43:30.993185Z",
     "iopub.status.busy": "2021-08-25T16:43:30.992552Z",
     "iopub.status.idle": "2021-08-25T16:43:30.997201Z",
     "shell.execute_reply": "2021-08-25T16:43:30.997743Z",
     "shell.execute_reply.started": "2021-08-25T16:23:49.362983Z"
    },
    "papermill": {
     "duration": 0.04465,
     "end_time": "2021-08-25T16:43:30.997929",
     "exception": false,
     "start_time": "2021-08-25T16:43:30.953279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Changes: Mean: 0.7251444688822899 Standard Deviation: 0.0033891436272372347\n",
      "Standardization: Mean: 0.7249719510461314 Standard Deviation: 0.0034370969646553865\n",
      "Normalization: Mean: 0.7395713351327234 Standard Deviation: 0.003302757768207594\n",
      "Log Transformation: Mean: 0.7249192465583121 Standard Deviation: 0.0035423241995017275\n",
      "Log Transformation + Standardization: Mean: 0.7509493735049747 Standard Deviation: 0.003024605939871177\n",
      "Standardization of One-Hot Encoding & Numerical Values: Mean: 0.7249719510461314 Standard Deviation: 0.0034370969646553865\n"
     ]
    }
   ],
   "source": [
    "print('No Changes: ' + 'Mean: ' + str(np.mean(scores)) + \" Standard Deviation: \" + str(np.std(scores)))\n",
    "print('Standardization: ' + 'Mean: ' + str(np.mean(scores_2)) + \" Standard Deviation: \" + str(np.std(scores_2)))\n",
    "print('Normalization: ' + 'Mean: ' + str(np.mean(scores_3)) + \" Standard Deviation: \" + str(np.std(scores_3)))\n",
    "print('Log Transformation: ' + 'Mean: ' + str(np.mean(scores_4)) + \" Standard Deviation: \" + str(np.std(scores_4)))\n",
    "print('Log Transformation + Standardization: ' + 'Mean: ' + str(np.mean(scores_5)) + \" Standard Deviation: \" + str(np.std(scores_5)))\n",
    "print('Standardization of One-Hot Encoding & Numerical Values: ' + 'Mean: ' + str(np.mean(scores_6)) + \" Standard Deviation: \" + str(np.std(scores_6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf732f37",
   "metadata": {
    "papermill": {
     "duration": 0.030979,
     "end_time": "2021-08-25T16:43:31.060013",
     "exception": false,
     "start_time": "2021-08-25T16:43:31.029034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We'll take the predictions of Log Transformation and make them the output for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae095d00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T16:43:31.125758Z",
     "iopub.status.busy": "2021-08-25T16:43:31.124947Z",
     "iopub.status.idle": "2021-08-25T16:43:31.132770Z",
     "shell.execute_reply": "2021-08-25T16:43:31.132348Z",
     "shell.execute_reply.started": "2021-08-25T16:30:03.716084Z"
    },
    "papermill": {
     "duration": 0.042375,
     "end_time": "2021-08-25T16:43:31.132880",
     "exception": false,
     "start_time": "2021-08-25T16:43:31.090505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = np.mean(np.column_stack(final_predictions_2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70a839ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T16:43:31.198926Z",
     "iopub.status.busy": "2021-08-25T16:43:31.198196Z",
     "iopub.status.idle": "2021-08-25T16:43:31.686777Z",
     "shell.execute_reply": "2021-08-25T16:43:31.686275Z",
     "shell.execute_reply.started": "2021-08-25T16:30:04.963430Z"
    },
    "papermill": {
     "duration": 0.52366,
     "end_time": "2021-08-25T16:43:31.686918",
     "exception": false,
     "start_time": "2021-08-25T16:43:31.163258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the predictions to a CSV file\n",
    "output = pd.DataFrame({'Id': X_test_full.id, 'target': predictions})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61d26b",
   "metadata": {
    "papermill": {
     "duration": 0.030188,
     "end_time": "2021-08-25T16:43:31.747896",
     "exception": false,
     "start_time": "2021-08-25T16:43:31.717708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Before we keep following Abhishek's tutorials, I want to implement LightGBM, as I've read it yields better results. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 125.72745,
   "end_time": "2021-08-25T16:43:33.254655",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-25T16:41:27.527205",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
