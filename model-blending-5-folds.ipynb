{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b0d252",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-08-31T20:36:58.359433Z",
     "iopub.status.busy": "2021-08-31T20:36:58.358902Z",
     "iopub.status.idle": "2021-08-31T20:36:59.393422Z",
     "shell.execute_reply": "2021-08-31T20:36:59.394022Z",
     "shell.execute_reply.started": "2021-08-31T20:17:59.311591Z"
    },
    "papermill": {
     "duration": 1.056636,
     "end_time": "2021-08-31T20:36:59.394320",
     "exception": false,
     "start_time": "2021-08-31T20:36:58.337684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_20_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_50_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_15_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_30_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_5_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_25_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_10_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_12_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_60_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_40_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_3_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_6_folds.csv\n",
      "/kaggle/input/30-days-of-ml/sample_submission.csv\n",
      "/kaggle/input/30-days-of-ml/train.csv\n",
      "/kaggle/input/30-days-of-ml/test.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# For one-hot encoding categorical variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# from sklearn.model_selection import train_test_split We won't be needing this anymore!\n",
    "\n",
    "# For the construction of the pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# For training the XGBoost model\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7264232",
   "metadata": {
    "papermill": {
     "duration": 0.007868,
     "end_time": "2021-08-31T20:36:59.411619",
     "exception": false,
     "start_time": "2021-08-31T20:36:59.403751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1st Model, Optimized XGBoost, Standardization, no Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5ce7ca0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T20:36:59.432197Z",
     "iopub.status.busy": "2021-08-31T20:36:59.431388Z",
     "iopub.status.idle": "2021-08-31T20:37:02.769707Z",
     "shell.execute_reply": "2021-08-31T20:37:02.767179Z",
     "shell.execute_reply.started": "2021-08-31T20:18:00.414238Z"
    },
    "papermill": {
     "duration": 3.350288,
     "end_time": "2021-08-31T20:37:02.769990",
     "exception": false,
     "start_time": "2021-08-31T20:36:59.419702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the training and test data. \n",
    "X_full = pd.read_csv(\"../input/train-folds-k-folds-30-days-of-ml/train_folds_5_folds.csv\")\n",
    "X_test_full = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\n",
    "sample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n",
    "fold_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a32fb41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T20:37:02.809759Z",
     "iopub.status.busy": "2021-08-31T20:37:02.805509Z",
     "iopub.status.idle": "2021-08-31T20:42:44.914987Z",
     "shell.execute_reply": "2021-08-31T20:42:44.914467Z",
     "shell.execute_reply.started": "2021-08-31T20:18:04.007869Z"
    },
    "papermill": {
     "duration": 342.134959,
     "end_time": "2021-08-31T20:42:44.915140",
     "exception": false,
     "start_time": "2021-08-31T20:37:02.780181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7167165958078671\n",
      "1 0.7242605532238721\n",
      "2 0.719839891484941\n",
      "3 0.7194734702971493\n",
      "4 0.7143128988919939\n",
      "0.7189206819411645 0.003339293541424143\n"
     ]
    }
   ],
   "source": [
    "# We select all features except \"id\", \"target\" and \"kfold\", as these are not predictors of our target.\n",
    "useful_features = [c for c in X_full.columns if c not in (\"id\", \"target\", \"kfold\")]\n",
    "\n",
    "# Select numerical columns by data type, not by column name\n",
    "num_cols = [col for col in X_full[useful_features] if X_full[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "# We select categorical columns. Note that we dropped the cardinality check.\n",
    "object_cols = [col for col in useful_features if 'cat' in col]\n",
    "\n",
    "# We build X_test out of X_test_full, but only selecting the useful features.\n",
    "X_test = X_test_full[useful_features]\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = preprocessing.StandardScaler()\n",
    "\n",
    "# Preprocessing for categorical data and one-hot encoding\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, num_cols),('cat', categorical_transformer, object_cols)])\n",
    "\n",
    "# Define the model \n",
    "model = XGBRegressor(tree_method='gpu_hist',\n",
    "                     gpu_id=0, \n",
    "                     predictor=\"gpu_predictor\",\n",
    "                     n_estimators = 25000,\n",
    "                     learning_rate=0.011185700021155284,\n",
    "                     reg_lambda=3.689489715666498e-07,\n",
    "                     reg_alpha=1.219041306467414e-05,\n",
    "                     subsample=0.5713235792096898,\n",
    "                     colsample_bytree=0.449446046,\n",
    "                     max_depth=3)\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
    "\n",
    "# We set up a list to store the final test and valid predictions.\n",
    "final_test_predictions = []\n",
    "final_valid_predictions = {}\n",
    "\n",
    "# We set up a list for storing the mean non squared error scores.\n",
    "scores = []\n",
    "\n",
    "# We set the loop to loop across all of the folds.\n",
    "for fold in range(fold_num):\n",
    "    X_train = X_full[X_full.kfold != fold].reset_index(drop=True) # We set the training data to be all folds different from the current fold number in the loop. We also reset the indices.\n",
    "    X_valid = X_full[X_full.kfold == fold].reset_index(drop=True) # The validation data is the current fold number in the loop. We also reset the indices.\n",
    "    X_test_copy = X_test.copy() # We copy the original X_test to not alter or overwrite over it.\n",
    "    \n",
    "    valid_ids = X_valid.id.values.tolist()\n",
    "    \n",
    "    y_train = X_train.target # We set the training target equal to the target in the training set. This has to be done every iteration (as the fold and the data changes).\n",
    "    y_valid = X_valid.target # We set the validation target equal to the target in the validation set. This has to be done every iteration (as the fold and the data changes).\n",
    "    \n",
    "    X_train = X_train[useful_features] # We set our training data to be the previously defined useful features of X_train.\n",
    "    X_valid = X_valid[useful_features] # We set our validation data to be the previously defined useful features of X_valid.\n",
    "    \n",
    "    # We activate the pipeline, which preprocesses the training data and fits the model (will take about 10 minutes to run)\n",
    "    my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    preds_valid = my_pipeline.predict(X_valid) # We instruct the pipeline to make predictions on X_valid.\n",
    "    preds_test = my_pipeline.predict(X_test) # We instruct the pipeline to make predictions on X_test.\n",
    "    \n",
    "    final_test_predictions.append(preds_test) # We append each of the test predictions on to our final_predictions list.\n",
    "    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n",
    "    \n",
    "    rmse = mean_squared_error(y_valid, preds_valid, squared=False) # We store the mean non squared error in a variable.\n",
    "    print(fold, rmse) # Print the fold number, and the mean non squared error for each fold.\n",
    "    scores.append(rmse) # We append the rmse value to the scores list.\n",
    "    \n",
    "print(np.mean(scores), np.std(scores)) # Print the mean non square error average, and its standard deviation\n",
    "\n",
    "final_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\n",
    "final_valid_predictions.columns = [\"id\", \"pred_1\"]\n",
    "final_valid_predictions.to_csv(\"train_pred_1.csv\", index=False)\n",
    "\n",
    "sample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\n",
    "sample_submission.columns = [\"id\", \"pred_1\"]\n",
    "sample_submission.to_csv(\"test_pred_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf116e2",
   "metadata": {
    "papermill": {
     "duration": 0.009223,
     "end_time": "2021-08-31T20:42:44.934398",
     "exception": false,
     "start_time": "2021-08-31T20:42:44.925175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2nd Model, Optimized XGBoost, Standardization, Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "553e1f86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T20:42:44.961477Z",
     "iopub.status.busy": "2021-08-31T20:42:44.956130Z",
     "iopub.status.idle": "2021-08-31T20:48:42.557312Z",
     "shell.execute_reply": "2021-08-31T20:48:42.556402Z",
     "shell.execute_reply.started": "2021-08-31T20:23:45.874333Z"
    },
    "papermill": {
     "duration": 357.613568,
     "end_time": "2021-08-31T20:48:42.557457",
     "exception": false,
     "start_time": "2021-08-31T20:42:44.943889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7167152692986495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.7246031988297222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.720042299057852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.7196938828011762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.714808988901577\n",
      "0.7191727277777954 0.0033346391640432715\n"
     ]
    }
   ],
   "source": [
    "# Load the training and test data. \n",
    "X_full = pd.read_csv(\"../input/train-folds-k-folds-30-days-of-ml/train_folds_5_folds.csv\")\n",
    "X_test_full = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\n",
    "sample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n",
    "fold_num = 5\n",
    "\n",
    "# We select all features except \"id\", \"target\" and \"kfold\", as these are not predictors of our target.\n",
    "useful_features = [c for c in X_full.columns if c not in (\"id\", \"target\", \"kfold\")]\n",
    "\n",
    "# Select numerical columns by data type, not by column name\n",
    "num_cols = [col for col in X_full[useful_features] if X_full[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "# We select categorical columns. Note that we dropped the cardinality check.\n",
    "object_cols = [col for col in useful_features if col.startswith(\"cat\")]\n",
    "\n",
    "# We build X_test out of X_test_full, but only selecting the useful features.\n",
    "X_test = X_test_full[useful_features]\n",
    "\n",
    "# Next up, we set up the for loop which will perform the target encoding:\n",
    "for col in object_cols: \n",
    "    temp_X_full = [] # We create a temporary list to store the dataframes.\n",
    "    temp_test_feature = None # We create a temporary feature for the test set.\n",
    "    \n",
    "    for fold in range(fold_num): # We loop across all folds\n",
    "        X_train = X_full[X_full.kfold != fold].reset_index(drop=True) \n",
    "        X_valid = X_full[X_full.kfold == fold].reset_index(drop=True) \n",
    "        feat = X_train.groupby(col)[\"target\"].agg(\"mean\") # We group the columns by target, and then we get the mean value of the values in \"target\" column.\n",
    "        feat = feat.to_dict() # We convert the dataframe into a dictionary.\n",
    "        X_valid.loc[:, f\"tar_enc_{col}\"] = X_valid[col].map(feat) # We map the mean values to a new column in X_valid.\n",
    "        temp_X_full.append(X_valid) # We append X_valid to our temporary list.\n",
    "        \n",
    "        if temp_test_feature is None: # If we don't have a temp_test_feature...\n",
    "            temp_test_feature = X_test[col].map(feat) # ...we assign it this value.\n",
    "            \n",
    "        else: # If its not None, (for folds above 0)...\n",
    "            temp_test_feature = temp_test_feature + X_test[col].map(feat) # ...add to it the present value.\n",
    "            \n",
    "    temp_test_feature = temp_test_feature/5 # We divide by the number of folds to get the average.\n",
    "    X_test.loc[:, f\"tar_enc_{col}\"] = temp_test_feature # We assign the temp_test_feat value to a new column.\n",
    "    X_full = pd.concat(temp_X_full) # We build the new X_full dataframe with the new target encoding columns.\n",
    "    \n",
    "# Preprocessing for numerical data, we use a StandardScaler to apply standardization.\n",
    "numerical_transformer = preprocessing.StandardScaler()\n",
    "\n",
    "# Preprocessing for categorical data and one-hot encoding.\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, num_cols),('cat', categorical_transformer, object_cols)])\n",
    "\n",
    "# Define the model \n",
    "model_2 = XGBRegressor(tree_method='gpu_hist',\n",
    "                     gpu_id=0, \n",
    "                     predictor=\"gpu_predictor\",\n",
    "                     n_estimators = 25000,\n",
    "                     learning_rate=0.03313934079213014,\n",
    "                     reg_lambda=7.795455194937734e-07,\n",
    "                     reg_alpha=11.375472681850685,\n",
    "                     subsample=0.8202458209691414,\n",
    "                     colsample_bytree=0.10071397127578051,\n",
    "                     max_depth=3)\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline_2 = Pipeline(steps=[('preprocessor', preprocessor), ('model', model_2)])\n",
    "\n",
    "# We set up a list to store the final test and valid predictions.\n",
    "final_test_predictions_2 = []\n",
    "final_valid_predictions_2 = {}\n",
    "\n",
    "# We set up a list for storing the mean non squared error scores.\n",
    "scores_2 = []\n",
    "\n",
    "# We set the loop to loop across all of the folds. Since we have 5 folds, the loop range will be range(5).\n",
    "for fold in range(fold_num):\n",
    "    X_train = X_full[X_full.kfold != fold].reset_index(drop=True) # We set the training data to be all folds different from the current fold number in the loop. We also reset the indices.\n",
    "    X_valid = X_full[X_full.kfold == fold].reset_index(drop=True) # The validation data is the current fold number in the loop. We also reset the indices.\n",
    "    X_test_copy = X_test.copy() # We copy the original X_test to not alter or overwrite over it.\n",
    "    \n",
    "    valid_ids = X_valid.id.values.tolist()\n",
    "    \n",
    "    y_train = X_train.target # We set the training target equal to the target in the training set. This has to be done every iteration (as the fold and the data changes).\n",
    "    y_valid = X_valid.target # We set the validation target equal to the target in the validation set. This has to be done every iteration (as the fold and the data changes).\n",
    "    \n",
    "    X_train = X_train[useful_features] # We set our training data to be the previously defined useful features of X_train.\n",
    "    X_valid = X_valid[useful_features] # We set our validation data to be the previously defined useful features of X_valid.\n",
    "    \n",
    "    # We activate the pipeline, which preprocesses the training data and fits the model (will take about 10 minutes to run)\n",
    "    my_pipeline_2.fit(X_train, y_train)\n",
    "\n",
    "    preds_valid = my_pipeline_2.predict(X_valid) # We instruct the pipeline to make predictions on X_valid.\n",
    "    preds_test = my_pipeline_2.predict(X_test) # We instruct the pipeline to make predictions on X_test.\n",
    "    \n",
    "    final_test_predictions_2.append(preds_test) # We append each of the test predictions on to our final_predictions list.\n",
    "    final_valid_predictions_2.update(dict(zip(valid_ids, preds_valid)))\n",
    "    \n",
    "    final_test_predictions_2.append(preds_test) # We append each of the test predictions on to our final_predictions list.\n",
    "    rmse = mean_squared_error(y_valid, preds_valid, squared=False) # We store the mean non squared error in a variable.\n",
    "    print(fold, rmse) # Print the fold number, and the mean non squared error for each fold.\n",
    "    scores_2.append(rmse) # We append the rmse value to the scores list.\n",
    "    \n",
    "print(np.mean(scores_2), np.std(scores_2)) # Print the mean non square error average, and its standard deviation\n",
    "\n",
    "final_valid_predictions_2 = pd.DataFrame.from_dict(final_valid_predictions_2, orient=\"index\").reset_index()\n",
    "final_valid_predictions_2.columns = [\"id\", \"pred_2\"]\n",
    "final_valid_predictions_2.to_csv(\"train_pred_2.csv\", index=False)\n",
    "\n",
    "sample_submission.target = np.mean(np.column_stack(final_test_predictions_2), axis=1)\n",
    "sample_submission.columns = [\"id\", \"pred_2\"]\n",
    "sample_submission.to_csv(\"test_pred_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc909530",
   "metadata": {
    "papermill": {
     "duration": 0.011801,
     "end_time": "2021-08-31T20:48:42.582662",
     "exception": false,
     "start_time": "2021-08-31T20:48:42.570861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model Blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "116b6d71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T20:48:42.612665Z",
     "iopub.status.busy": "2021-08-31T20:48:42.612042Z",
     "iopub.status.idle": "2021-08-31T20:48:44.797430Z",
     "shell.execute_reply": "2021-08-31T20:48:44.796673Z",
     "shell.execute_reply.started": "2021-08-31T20:29:44.764794Z"
    },
    "papermill": {
     "duration": 2.203015,
     "end_time": "2021-08-31T20:48:44.797551",
     "exception": false,
     "start_time": "2021-08-31T20:48:42.594536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>...</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>target</th>\n",
       "      <th>kfold</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>pred_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389470</td>\n",
       "      <td>0.267559</td>\n",
       "      <td>0.237281</td>\n",
       "      <td>0.377873</td>\n",
       "      <td>0.322401</td>\n",
       "      <td>0.869850</td>\n",
       "      <td>8.113634</td>\n",
       "      <td>4</td>\n",
       "      <td>8.546576</td>\n",
       "      <td>8.532659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594928</td>\n",
       "      <td>0.341439</td>\n",
       "      <td>0.906013</td>\n",
       "      <td>0.921701</td>\n",
       "      <td>0.261975</td>\n",
       "      <td>0.465083</td>\n",
       "      <td>8.481233</td>\n",
       "      <td>0</td>\n",
       "      <td>8.479118</td>\n",
       "      <td>8.399091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555205</td>\n",
       "      <td>0.843531</td>\n",
       "      <td>0.748809</td>\n",
       "      <td>0.620126</td>\n",
       "      <td>0.541474</td>\n",
       "      <td>0.763846</td>\n",
       "      <td>8.364351</td>\n",
       "      <td>4</td>\n",
       "      <td>8.209414</td>\n",
       "      <td>8.246557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.679618</td>\n",
       "      <td>0.574844</td>\n",
       "      <td>0.346010</td>\n",
       "      <td>0.714610</td>\n",
       "      <td>0.540150</td>\n",
       "      <td>0.280682</td>\n",
       "      <td>8.049253</td>\n",
       "      <td>1</td>\n",
       "      <td>8.412892</td>\n",
       "      <td>8.476482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684501</td>\n",
       "      <td>0.956692</td>\n",
       "      <td>1.000773</td>\n",
       "      <td>0.776742</td>\n",
       "      <td>0.625849</td>\n",
       "      <td>0.250823</td>\n",
       "      <td>7.972260</td>\n",
       "      <td>0</td>\n",
       "      <td>8.198384</td>\n",
       "      <td>8.276423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8  ...     cont8     cont9  \\\n",
       "0   1    B    B    B    C    B    B    A    E    C  ...  0.389470  0.267559   \n",
       "1   2    B    B    A    A    B    D    A    F    A  ...  0.594928  0.341439   \n",
       "2   3    A    A    A    C    B    D    A    D    A  ...  0.555205  0.843531   \n",
       "3   4    B    B    A    C    B    D    A    E    C  ...  0.679618  0.574844   \n",
       "4   6    A    A    A    C    B    D    A    E    A  ...  0.684501  0.956692   \n",
       "\n",
       "     cont10    cont11    cont12    cont13    target  kfold    pred_1    pred_2  \n",
       "0  0.237281  0.377873  0.322401  0.869850  8.113634      4  8.546576  8.532659  \n",
       "1  0.906013  0.921701  0.261975  0.465083  8.481233      0  8.479118  8.399091  \n",
       "2  0.748809  0.620126  0.541474  0.763846  8.364351      4  8.209414  8.246557  \n",
       "3  0.346010  0.714610  0.540150  0.280682  8.049253      1  8.412892  8.476482  \n",
       "4  1.000773  0.776742  0.625849  0.250823  7.972260      0  8.198384  8.276423  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full = pd.read_csv(\"../input/train-folds-k-folds-30-days-of-ml/train_folds_5_folds.csv\")\n",
    "X_test_full = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\n",
    "sample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n",
    "\n",
    "df1 = pd.read_csv(\"train_pred_1.csv\")\n",
    "df2 = pd.read_csv(\"train_pred_2.csv\")\n",
    "\n",
    "df_test1 = pd.read_csv(\"test_pred_1.csv\")\n",
    "df_test2 = pd.read_csv(\"test_pred_2.csv\")\n",
    "\n",
    "X_full = X_full.merge(df1, on=\"id\", how=\"left\")\n",
    "X_full = X_full.merge(df2, on=\"id\", how=\"left\")\n",
    "\n",
    "X_test_full = X_test_full.merge(df_test1, on=\"id\", how=\"left\")\n",
    "X_test_full = X_test_full.merge(df_test2, on=\"id\", how=\"left\")\n",
    "\n",
    "X_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a249828",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T20:48:44.832179Z",
     "iopub.status.busy": "2021-08-31T20:48:44.830730Z",
     "iopub.status.idle": "2021-08-31T20:48:45.579170Z",
     "shell.execute_reply": "2021-08-31T20:48:45.580292Z",
     "shell.execute_reply.started": "2021-08-31T20:35:43.607758Z"
    },
    "papermill": {
     "duration": 0.770132,
     "end_time": "2021-08-31T20:48:45.580519",
     "exception": false,
     "start_time": "2021-08-31T20:48:44.810387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.716173967617178\n",
      "1 0.7239638867362743\n",
      "2 0.7193624013109371\n",
      "3 0.7189656679857905\n",
      "4 0.7140025854918751\n",
      "0.7184937018284111 0.0033601073354859548\n"
     ]
    }
   ],
   "source": [
    "useful_features = [\"pred_1\", \"pred_2\"]\n",
    "X_test_full = X_test_full[useful_features]\n",
    "\n",
    "final_predictions = []\n",
    "scores = []\n",
    "for fold in range(5):\n",
    "    X_train = X_full[X_full.kfold != fold].reset_index(drop=True) # We set the training data to be all folds different from the current fold number in the loop. We also reset the indices.\n",
    "    X_valid = X_full[X_full.kfold == fold].reset_index(drop=True) # The validation data is the current fold number in the loop. We also reset the indices.\n",
    "    X_test = X_test_full.copy() # We copy the original X_test to not alter or overwrite over it.\n",
    "    \n",
    "    y_train = X_train.target # We set the training target equal to the target in the training set. This has to be done every iteration (as the fold and the data changes).\n",
    "    y_valid = X_valid.target # We set the validation target equal to the target in the validation set. This has to be done every iteration (as the fold and the data changes).\n",
    "    \n",
    "    X_train = X_train[useful_features] # We set our training data to be the previously defined useful features of X_train.\n",
    "    X_valid = X_valid[useful_features] # We set our validation data to be the previously defined useful features of X_valid.\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    preds_valid = model.predict(X_valid)\n",
    "    test_preds = model.predict(X_test)\n",
    "    final_predictions.append(test_preds)\n",
    "    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n",
    "    print(fold, rmse)\n",
    "    scores.append(rmse)\n",
    "\n",
    "print(np.mean(scores), np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f91a8d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T20:48:45.643555Z",
     "iopub.status.busy": "2021-08-31T20:48:45.642747Z",
     "iopub.status.idle": "2021-08-31T20:48:46.280198Z",
     "shell.execute_reply": "2021-08-31T20:48:46.278270Z",
     "shell.execute_reply.started": "2021-08-31T20:35:51.947310Z"
    },
    "papermill": {
     "duration": 0.674304,
     "end_time": "2021-08-31T20:48:46.280393",
     "exception": false,
     "start_time": "2021-08-31T20:48:45.606089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_submission.target = np.mean(np.column_stack(final_predictions), axis=1)\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a0c5af",
   "metadata": {
    "papermill": {
     "duration": 0.022022,
     "end_time": "2021-08-31T20:48:46.329899",
     "exception": false,
     "start_time": "2021-08-31T20:48:46.307877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 715.210415,
   "end_time": "2021-08-31T20:48:47.065003",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-31T20:36:51.854588",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
