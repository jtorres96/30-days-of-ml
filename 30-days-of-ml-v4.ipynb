{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b41c4ac",
   "metadata": {
    "papermill": {
     "duration": 0.011214,
     "end_time": "2021-08-23T20:38:44.687856",
     "exception": false,
     "start_time": "2021-08-23T20:38:44.676642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 30 Days of ML v4, by Juan Torres\n",
    "### Based of my previous notebook, 30 Days of ML v3: https://www.kaggle.com/jtorres96/30-days-of-ml-v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c796a6",
   "metadata": {
    "papermill": {
     "duration": 0.009624,
     "end_time": "2021-08-23T20:38:44.707791",
     "exception": false,
     "start_time": "2021-08-23T20:38:44.698167",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook will be an iteration on the previous notebook. Previously, we found out that we get a better result from one-hot encoding. In this version, we will use a different model from the Random Forest we have used up until now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcaa075",
   "metadata": {
    "papermill": {
     "duration": 0.01075,
     "end_time": "2021-08-23T20:38:44.728216",
     "exception": false,
     "start_time": "2021-08-23T20:38:44.717466",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Importing libraries\n",
    "First, let's import the libraries we'll use for this first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e3834f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-08-23T20:38:44.775008Z",
     "iopub.status.busy": "2021-08-23T20:38:44.773482Z",
     "iopub.status.idle": "2021-08-23T20:38:45.930089Z",
     "shell.execute_reply": "2021-08-23T20:38:45.929200Z",
     "shell.execute_reply.started": "2021-08-23T20:23:50.407743Z"
    },
    "papermill": {
     "duration": 1.185282,
     "end_time": "2021-08-23T20:38:45.930240",
     "exception": false,
     "start_time": "2021-08-23T20:38:44.744958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/30-days-of-ml/sample_submission.csv\n",
      "/kaggle/input/30-days-of-ml/train.csv\n",
      "/kaggle/input/30-days-of-ml/test.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# For encoding categorical variables, splitting data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For the construction of the pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# For training the XGBoost model\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95362617",
   "metadata": {
    "papermill": {
     "duration": 0.009874,
     "end_time": "2021-08-23T20:38:45.950636",
     "exception": false,
     "start_time": "2021-08-23T20:38:45.940762",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Loading and preparing data\n",
    "Next, let's load the training and test data, separate our target from the training features, separate categorical and numerical columns and do a train_test_split to break off a validation set from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47a084d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T20:38:45.977321Z",
     "iopub.status.busy": "2021-08-23T20:38:45.976807Z",
     "iopub.status.idle": "2021-08-23T20:38:50.437419Z",
     "shell.execute_reply": "2021-08-23T20:38:50.436903Z",
     "shell.execute_reply.started": "2021-08-23T20:23:51.622272Z"
    },
    "papermill": {
     "duration": 4.476913,
     "end_time": "2021-08-23T20:38:50.437554",
     "exception": false,
     "start_time": "2021-08-23T20:38:45.960641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cont0</th>\n",
       "      <th>...</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>412957</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>G</td>\n",
       "      <td>1.035400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284988</td>\n",
       "      <td>0.777432</td>\n",
       "      <td>0.391158</td>\n",
       "      <td>0.575325</td>\n",
       "      <td>0.449008</td>\n",
       "      <td>0.808493</td>\n",
       "      <td>0.638874</td>\n",
       "      <td>0.620083</td>\n",
       "      <td>0.860327</td>\n",
       "      <td>0.796654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282449</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>0.552942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979303</td>\n",
       "      <td>0.388542</td>\n",
       "      <td>0.392169</td>\n",
       "      <td>0.251943</td>\n",
       "      <td>0.257683</td>\n",
       "      <td>0.289205</td>\n",
       "      <td>0.753284</td>\n",
       "      <td>0.108515</td>\n",
       "      <td>0.223745</td>\n",
       "      <td>0.867225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164867</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>F</td>\n",
       "      <td>0.300135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528495</td>\n",
       "      <td>0.877585</td>\n",
       "      <td>0.692778</td>\n",
       "      <td>0.394110</td>\n",
       "      <td>0.372842</td>\n",
       "      <td>0.603872</td>\n",
       "      <td>0.729337</td>\n",
       "      <td>0.572204</td>\n",
       "      <td>0.381617</td>\n",
       "      <td>0.528454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>0.423174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795973</td>\n",
       "      <td>0.254421</td>\n",
       "      <td>0.279979</td>\n",
       "      <td>0.244269</td>\n",
       "      <td>0.549558</td>\n",
       "      <td>0.318729</td>\n",
       "      <td>0.093974</td>\n",
       "      <td>0.428488</td>\n",
       "      <td>0.176486</td>\n",
       "      <td>0.250280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80790</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>A</td>\n",
       "      <td>0.667090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809695</td>\n",
       "      <td>0.412479</td>\n",
       "      <td>0.345747</td>\n",
       "      <td>0.537579</td>\n",
       "      <td>1.023082</td>\n",
       "      <td>0.433388</td>\n",
       "      <td>0.814753</td>\n",
       "      <td>0.655539</td>\n",
       "      <td>0.923203</td>\n",
       "      <td>0.467355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8     cont0  ...     cont4  \\\n",
       "id                                                             ...             \n",
       "412957    A    A    A    C    B    C    A    D    G  1.035400  ...  0.284988   \n",
       "282449    A    A    A    C    B    B    A    E    E  0.552942  ...  0.979303   \n",
       "164867    A    A    A    A    B    B    A    E    F  0.300135  ...  0.528495   \n",
       "541       A    B    A    C    B    B    A    E    E  0.423174  ...  0.795973   \n",
       "80790     B    B    A    C    B    D    A    E    A  0.667090  ...  0.809695   \n",
       "\n",
       "           cont5     cont6     cont7     cont8     cont9    cont10    cont11  \\\n",
       "id                                                                             \n",
       "412957  0.777432  0.391158  0.575325  0.449008  0.808493  0.638874  0.620083   \n",
       "282449  0.388542  0.392169  0.251943  0.257683  0.289205  0.753284  0.108515   \n",
       "164867  0.877585  0.692778  0.394110  0.372842  0.603872  0.729337  0.572204   \n",
       "541     0.254421  0.279979  0.244269  0.549558  0.318729  0.093974  0.428488   \n",
       "80790   0.412479  0.345747  0.537579  1.023082  0.433388  0.814753  0.655539   \n",
       "\n",
       "          cont12    cont13  \n",
       "id                          \n",
       "412957  0.860327  0.796654  \n",
       "282449  0.223745  0.867225  \n",
       "164867  0.381617  0.528454  \n",
       "541     0.176486  0.250280  \n",
       "80790   0.923203  0.467355  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training and test data. We set index_col=0 in the code cell below to use the id column to index the DataFrame. \n",
    "X_full = pd.read_csv(\"../input/30-days-of-ml/train.csv\", index_col=0)\n",
    "X_test_full = pd.read_csv(\"../input/30-days-of-ml/test.csv\", index_col=0)\n",
    "\n",
    "# Separate target (designated \"y\") from the training features.\n",
    "y = X_full['target']\n",
    "X = X_full.drop(['target'], axis=1)\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Select categorical columns with relatively low cardinality\n",
    "object_cols = [col for col in X.columns if 'cat' in col and X[col].nunique() < 10] # We are separating the categorical columns by the column name, previously we had done this by checking the data type of each column.\n",
    "\n",
    "# Select numerical columns\n",
    "num_cols = [col for col in X.columns if 'cont' in col]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = object_cols + num_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "X_test = X_test_full[my_cols].copy()\n",
    "\n",
    "# Take a peek at the training data\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196377ad",
   "metadata": {
    "papermill": {
     "duration": 0.010542,
     "end_time": "2021-08-23T20:38:50.460409",
     "exception": false,
     "start_time": "2021-08-23T20:38:50.449867",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Building the pipeline\n",
    "### 3.1: Define Preprocessing Steps\n",
    "We'll use the ColumnTransformer class to bundle together different preprocessing steps. The code below will impute missing values in numerical and categorical data, and apply a **one-hot encoding** to categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "625f9a78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T20:38:50.487710Z",
     "iopub.status.busy": "2021-08-23T20:38:50.486455Z",
     "iopub.status.idle": "2021-08-23T20:38:50.488846Z",
     "shell.execute_reply": "2021-08-23T20:38:50.489247Z",
     "shell.execute_reply.started": "2021-08-23T20:23:55.714507Z"
    },
    "papermill": {
     "duration": 0.018145,
     "end_time": "2021-08-23T20:38:50.489384",
     "exception": false,
     "start_time": "2021-08-23T20:38:50.471239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, num_cols),('cat', categorical_transformer, object_cols)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c8e2a",
   "metadata": {
    "papermill": {
     "duration": 0.010348,
     "end_time": "2021-08-23T20:38:50.510581",
     "exception": false,
     "start_time": "2021-08-23T20:38:50.500233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.2: Define the Model\n",
    "Next, we define a **gradient boost** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b998c4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T20:38:50.535779Z",
     "iopub.status.busy": "2021-08-23T20:38:50.535220Z",
     "iopub.status.idle": "2021-08-23T20:38:50.539055Z",
     "shell.execute_reply": "2021-08-23T20:38:50.538610Z",
     "shell.execute_reply.started": "2021-08-23T20:23:55.721867Z"
    },
    "papermill": {
     "duration": 0.017776,
     "end_time": "2021-08-23T20:38:50.539165",
     "exception": false,
     "start_time": "2021-08-23T20:38:50.521389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model \n",
    "model = XGBRegressor(random_state=1, tree_method = 'gpu_hist') # We set this to make the XGBoost model run on GPU, as it speed up the fitting significatively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cb3c56",
   "metadata": {
    "papermill": {
     "duration": 0.01037,
     "end_time": "2021-08-23T20:38:50.560253",
     "exception": false,
     "start_time": "2021-08-23T20:38:50.549883",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.3: Create and Evaluate the Pipeline\n",
    "Finally, we use the Pipeline class to define a pipeline that bundles the preprocessing and modeling steps. Let's remember two important things:\n",
    "* With the pipeline, we preprocess the training data and fit the model in a single line of code. (In contrast, without a pipeline, we have to do imputation, one-hot encoding, and model training in separate steps. This becomes especially messy if we have to deal with both numerical and categorical variables!)\n",
    "* With the pipeline, we supply the unprocessed features in X_valid to the predict() command, and the pipeline automatically preprocesses the features before generating predictions. (However, without a pipeline, we have to remember to preprocess the validation data before making predictions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7988eb14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T20:38:50.595029Z",
     "iopub.status.busy": "2021-08-23T20:38:50.594078Z",
     "iopub.status.idle": "2021-08-23T20:38:54.624211Z",
     "shell.execute_reply": "2021-08-23T20:38:54.624697Z",
     "shell.execute_reply.started": "2021-08-23T20:23:59.358684Z"
    },
    "papermill": {
     "duration": 4.053845,
     "end_time": "2021-08-23T20:38:54.624843",
     "exception": false,
     "start_time": "2021-08-23T20:38:50.570998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7277746473192207\n"
     ]
    }
   ],
   "source": [
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
    "\n",
    "# Preprocessing of training data, fit model (will take about 10 minutes to run)\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds_valid = my_pipeline.predict(X_valid)\n",
    "\n",
    "print(mean_squared_error(y_valid, preds_valid, squared=False)) # We set squared=False to get the root mean squared error (RMSE) on the validation data, as this is the competition's requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec18a8",
   "metadata": {
    "papermill": {
     "duration": 0.010941,
     "end_time": "2021-08-23T20:38:54.646874",
     "exception": false,
     "start_time": "2021-08-23T20:38:54.635933",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Previous score was 0.73826, let's compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10c7deec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T20:38:54.673763Z",
     "iopub.status.busy": "2021-08-23T20:38:54.672876Z",
     "iopub.status.idle": "2021-08-23T20:38:54.677079Z",
     "shell.execute_reply": "2021-08-23T20:38:54.677515Z",
     "shell.execute_reply.started": "2021-08-23T20:24:54.541091Z"
    },
    "papermill": {
     "duration": 0.019769,
     "end_time": "2021-08-23T20:38:54.677640",
     "exception": false,
     "start_time": "2021-08-23T20:38:54.657871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010485352680779325\n"
     ]
    }
   ],
   "source": [
    "print(abs(0.73826-mean_squared_error(y_valid, preds_valid, squared=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8629f8",
   "metadata": {
    "papermill": {
     "duration": 0.011245,
     "end_time": "2021-08-23T20:38:54.699969",
     "exception": false,
     "start_time": "2021-08-23T20:38:54.688724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The gradient boost model is a considerable improvement over the random forest model for the following two reasons:\n",
    "* We have achieved a lower (and thus better) mean squared error.\n",
    "* The time to run this model is much, much lower! While a random forest took up to 15 minutes to run on my machine, this model ran in under a minute, thanks to the GPU boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfb0089",
   "metadata": {
    "papermill": {
     "duration": 0.011087,
     "end_time": "2021-08-23T20:38:54.722226",
     "exception": false,
     "start_time": "2021-08-23T20:38:54.711139",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Submit predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46ef2e0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T20:38:54.761220Z",
     "iopub.status.busy": "2021-08-23T20:38:54.760263Z",
     "iopub.status.idle": "2021-08-23T20:38:56.342487Z",
     "shell.execute_reply": "2021-08-23T20:38:56.341929Z",
     "shell.execute_reply.started": "2021-08-23T20:37:57.629018Z"
    },
    "papermill": {
     "duration": 1.608981,
     "end_time": "2021-08-23T20:38:56.342625",
     "exception": false,
     "start_time": "2021-08-23T20:38:54.733644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the model to generate predictions\n",
    "predictions = my_pipeline.predict(X_test)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "output = pd.DataFrame({'Id': X_test.index,\n",
    "                       'target': predictions})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b57ceb",
   "metadata": {
    "papermill": {
     "duration": 0.011461,
     "end_time": "2021-08-23T20:38:56.366474",
     "exception": false,
     "start_time": "2021-08-23T20:38:56.355013",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We have achieved an acceptable baseline from which we can achieve much better results. A roadmap for the next versions:\n",
    "* Look into tuning our model via hyperparameter optimization (apparently there is a tool that will help us do this, called Optuna)\n",
    "* Look into the data for correlation (or lack of), so we can properly identify important features in our model.\n",
    "* Look into crossvalidation.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.586889,
   "end_time": "2021-08-23T20:38:57.886174",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-23T20:38:38.299285",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
