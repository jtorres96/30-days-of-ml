{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22e81eec",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-08-31T20:57:18.590363Z",
     "iopub.status.busy": "2021-08-31T20:57:18.588364Z",
     "iopub.status.idle": "2021-08-31T20:57:19.905794Z",
     "shell.execute_reply": "2021-08-31T20:57:19.904780Z",
     "shell.execute_reply.started": "2021-08-31T20:55:20.016172Z"
    },
    "papermill": {
     "duration": 1.333995,
     "end_time": "2021-08-31T20:57:19.905973",
     "exception": false,
     "start_time": "2021-08-31T20:57:18.571978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/30-days-of-ml/sample_submission.csv\n",
      "/kaggle/input/30-days-of-ml/train.csv\n",
      "/kaggle/input/30-days-of-ml/test.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_20_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_50_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_15_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_30_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_5_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_25_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_10_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_12_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_60_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_40_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_3_folds.csv\n",
      "/kaggle/input/train-folds-k-folds-30-days-of-ml/train_folds_6_folds.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# For one-hot encoding categorical variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# from sklearn.model_selection import train_test_split We won't be needing this anymore!\n",
    "\n",
    "# For the construction of the pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# For training the XGBoost model\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29304ce5",
   "metadata": {
    "papermill": {
     "duration": 0.009428,
     "end_time": "2021-08-31T20:57:19.927283",
     "exception": false,
     "start_time": "2021-08-31T20:57:19.917855",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1st Model, Optimized XGBoost, Standardization, no Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee5edf24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T20:57:19.954098Z",
     "iopub.status.busy": "2021-08-31T20:57:19.953282Z",
     "iopub.status.idle": "2021-08-31T20:57:24.025611Z",
     "shell.execute_reply": "2021-08-31T20:57:24.024498Z",
     "shell.execute_reply.started": "2021-08-31T20:55:21.045337Z"
    },
    "papermill": {
     "duration": 4.08806,
     "end_time": "2021-08-31T20:57:24.025778",
     "exception": false,
     "start_time": "2021-08-31T20:57:19.937718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the training and test data. \n",
    "fold_num = 10\n",
    "X_full = pd.read_csv(\"../input/train-folds-k-folds-30-days-of-ml/train_folds_\"+ str(fold_num)+ \"_folds.csv\")\n",
    "X_test_full = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\n",
    "sample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e80410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T20:57:24.088757Z",
     "iopub.status.busy": "2021-08-31T20:57:24.055625Z",
     "iopub.status.idle": "2021-08-31T21:10:12.764347Z",
     "shell.execute_reply": "2021-08-31T21:10:12.763731Z",
     "shell.execute_reply.started": "2021-08-31T20:18:04.007869Z"
    },
    "papermill": {
     "duration": 768.727916,
     "end_time": "2021-08-31T21:10:12.764517",
     "exception": false,
     "start_time": "2021-08-31T20:57:24.036601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7155336816958349\n",
      "1 0.716993928592888\n",
      "2 0.7206393577105712\n",
      "3 0.7273639518846338\n",
      "4 0.7203349565079591\n",
      "5 0.7186480903753499\n",
      "6 0.7173526913226549\n",
      "7 0.7210420259736751\n",
      "8 0.7162417362246509\n",
      "9 0.7120861252032764\n",
      "0.7186236545491494 0.0038918586806434455\n"
     ]
    }
   ],
   "source": [
    "# We select all features except \"id\", \"target\" and \"kfold\", as these are not predictors of our target.\n",
    "useful_features = [c for c in X_full.columns if c not in (\"id\", \"target\", \"kfold\")]\n",
    "\n",
    "# Select numerical columns by data type, not by column name\n",
    "num_cols = [col for col in X_full[useful_features] if X_full[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "# We select categorical columns. Note that we dropped the cardinality check.\n",
    "object_cols = [col for col in useful_features if 'cat' in col]\n",
    "\n",
    "# We build X_test out of X_test_full, but only selecting the useful features.\n",
    "X_test = X_test_full[useful_features]\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = preprocessing.StandardScaler()\n",
    "\n",
    "# Preprocessing for categorical data and one-hot encoding\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, num_cols),('cat', categorical_transformer, object_cols)])\n",
    "\n",
    "# Define the model \n",
    "model = XGBRegressor(tree_method='gpu_hist',\n",
    "                     gpu_id=0, \n",
    "                     predictor=\"gpu_predictor\",\n",
    "                     n_estimators = 25000,\n",
    "                     learning_rate=0.011185700021155284,\n",
    "                     reg_lambda=3.689489715666498e-07,\n",
    "                     reg_alpha=1.219041306467414e-05,\n",
    "                     subsample=0.5713235792096898,\n",
    "                     colsample_bytree=0.449446046,\n",
    "                     max_depth=3)\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
    "\n",
    "# We set up a list to store the final test and valid predictions.\n",
    "final_test_predictions = []\n",
    "final_valid_predictions = {}\n",
    "\n",
    "# We set up a list for storing the mean non squared error scores.\n",
    "scores = []\n",
    "\n",
    "# We set the loop to loop across all of the folds.\n",
    "for fold in range(fold_num):\n",
    "    X_train = X_full[X_full.kfold != fold].reset_index(drop=True) # We set the training data to be all folds different from the current fold number in the loop. We also reset the indices.\n",
    "    X_valid = X_full[X_full.kfold == fold].reset_index(drop=True) # The validation data is the current fold number in the loop. We also reset the indices.\n",
    "    X_test_copy = X_test.copy() # We copy the original X_test to not alter or overwrite over it.\n",
    "    \n",
    "    valid_ids = X_valid.id.values.tolist()\n",
    "    \n",
    "    y_train = X_train.target # We set the training target equal to the target in the training set. This has to be done every iteration (as the fold and the data changes).\n",
    "    y_valid = X_valid.target # We set the validation target equal to the target in the validation set. This has to be done every iteration (as the fold and the data changes).\n",
    "    \n",
    "    X_train = X_train[useful_features] # We set our training data to be the previously defined useful features of X_train.\n",
    "    X_valid = X_valid[useful_features] # We set our validation data to be the previously defined useful features of X_valid.\n",
    "    \n",
    "    # We activate the pipeline, which preprocesses the training data and fits the model (will take about 10 minutes to run)\n",
    "    my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    preds_valid = my_pipeline.predict(X_valid) # We instruct the pipeline to make predictions on X_valid.\n",
    "    preds_test = my_pipeline.predict(X_test) # We instruct the pipeline to make predictions on X_test.\n",
    "    \n",
    "    final_test_predictions.append(preds_test) # We append each of the test predictions on to our final_predictions list.\n",
    "    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n",
    "    \n",
    "    rmse = mean_squared_error(y_valid, preds_valid, squared=False) # We store the mean non squared error in a variable.\n",
    "    print(fold, rmse) # Print the fold number, and the mean non squared error for each fold.\n",
    "    scores.append(rmse) # We append the rmse value to the scores list.\n",
    "    \n",
    "print(np.mean(scores), np.std(scores)) # Print the mean non square error average, and its standard deviation\n",
    "\n",
    "final_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\n",
    "final_valid_predictions.columns = [\"id\", \"pred_1\"]\n",
    "final_valid_predictions.to_csv(\"train_pred_1.csv\", index=False)\n",
    "\n",
    "sample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\n",
    "sample_submission.columns = [\"id\", \"pred_1\"]\n",
    "sample_submission.to_csv(\"test_pred_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd07e6",
   "metadata": {
    "papermill": {
     "duration": 0.013184,
     "end_time": "2021-08-31T21:10:12.791598",
     "exception": false,
     "start_time": "2021-08-31T21:10:12.778414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2nd Model, Optimized XGBoost, Standardization, Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0c5e5fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T21:10:12.840417Z",
     "iopub.status.busy": "2021-08-31T21:10:12.832462Z",
     "iopub.status.idle": "2021-08-31T21:23:37.918093Z",
     "shell.execute_reply": "2021-08-31T21:23:37.917289Z",
     "shell.execute_reply.started": "2021-08-31T20:23:45.874333Z"
    },
    "papermill": {
     "duration": 805.113154,
     "end_time": "2021-08-31T21:23:37.918331",
     "exception": false,
     "start_time": "2021-08-31T21:10:12.805177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7156060662672703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.717208611267233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.7209856692202776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.7276157070892456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.7204396971547282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.718747089188121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0.7173712429619467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 0.7212983795269985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.7165080570375956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0.712405897990994\n",
      "0.718818641770441 0.003903028226237659\n"
     ]
    }
   ],
   "source": [
    "# Load the training and test data. \n",
    "X_full = pd.read_csv(\"../input/train-folds-k-folds-30-days-of-ml/train_folds_\"+ str(fold_num)+ \"_folds.csv\")\n",
    "X_test_full = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\n",
    "sample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n",
    "\n",
    "# We select all features except \"id\", \"target\" and \"kfold\", as these are not predictors of our target.\n",
    "useful_features = [c for c in X_full.columns if c not in (\"id\", \"target\", \"kfold\")]\n",
    "\n",
    "# Select numerical columns by data type, not by column name\n",
    "num_cols = [col for col in X_full[useful_features] if X_full[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "# We select categorical columns. Note that we dropped the cardinality check.\n",
    "object_cols = [col for col in useful_features if col.startswith(\"cat\")]\n",
    "\n",
    "# We build X_test out of X_test_full, but only selecting the useful features.\n",
    "X_test = X_test_full[useful_features]\n",
    "\n",
    "# Next up, we set up the for loop which will perform the target encoding:\n",
    "for col in object_cols: \n",
    "    temp_X_full = [] # We create a temporary list to store the dataframes.\n",
    "    temp_test_feature = None # We create a temporary feature for the test set.\n",
    "    \n",
    "    for fold in range(fold_num): # We loop across all folds\n",
    "        X_train = X_full[X_full.kfold != fold].reset_index(drop=True) \n",
    "        X_valid = X_full[X_full.kfold == fold].reset_index(drop=True) \n",
    "        feat = X_train.groupby(col)[\"target\"].agg(\"mean\") # We group the columns by target, and then we get the mean value of the values in \"target\" column.\n",
    "        feat = feat.to_dict() # We convert the dataframe into a dictionary.\n",
    "        X_valid.loc[:, f\"tar_enc_{col}\"] = X_valid[col].map(feat) # We map the mean values to a new column in X_valid.\n",
    "        temp_X_full.append(X_valid) # We append X_valid to our temporary list.\n",
    "        \n",
    "        if temp_test_feature is None: # If we don't have a temp_test_feature...\n",
    "            temp_test_feature = X_test[col].map(feat) # ...we assign it this value.\n",
    "            \n",
    "        else: # If its not None, (for folds above 0)...\n",
    "            temp_test_feature = temp_test_feature + X_test[col].map(feat) # ...add to it the present value.\n",
    "            \n",
    "    temp_test_feature = temp_test_feature/fold_num # We divide by the number of folds to get the average.\n",
    "    X_test.loc[:, f\"tar_enc_{col}\"] = temp_test_feature # We assign the temp_test_feat value to a new column.\n",
    "    X_full = pd.concat(temp_X_full) # We build the new X_full dataframe with the new target encoding columns.\n",
    "    \n",
    "# Preprocessing for numerical data, we use a StandardScaler to apply standardization.\n",
    "numerical_transformer = preprocessing.StandardScaler()\n",
    "\n",
    "# Preprocessing for categorical data and one-hot encoding.\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, num_cols),('cat', categorical_transformer, object_cols)])\n",
    "\n",
    "# Define the model \n",
    "model_2 = XGBRegressor(tree_method='gpu_hist',\n",
    "                     gpu_id=0, \n",
    "                     predictor=\"gpu_predictor\",\n",
    "                     n_estimators = 25000,\n",
    "                     learning_rate=0.03313934079213014,\n",
    "                     reg_lambda=7.795455194937734e-07,\n",
    "                     reg_alpha=11.375472681850685,\n",
    "                     subsample=0.8202458209691414,\n",
    "                     colsample_bytree=0.10071397127578051,\n",
    "                     max_depth=3)\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline_2 = Pipeline(steps=[('preprocessor', preprocessor), ('model', model_2)])\n",
    "\n",
    "# We set up a list to store the final test and valid predictions.\n",
    "final_test_predictions_2 = []\n",
    "final_valid_predictions_2 = {}\n",
    "\n",
    "# We set up a list for storing the mean non squared error scores.\n",
    "scores_2 = []\n",
    "\n",
    "# We set the loop to loop across all of the folds.\n",
    "for fold in range(fold_num):\n",
    "    X_train = X_full[X_full.kfold != fold].reset_index(drop=True) # We set the training data to be all folds different from the current fold number in the loop. We also reset the indices.\n",
    "    X_valid = X_full[X_full.kfold == fold].reset_index(drop=True) # The validation data is the current fold number in the loop. We also reset the indices.\n",
    "    X_test_copy = X_test.copy() # We copy the original X_test to not alter or overwrite over it.\n",
    "    \n",
    "    valid_ids = X_valid.id.values.tolist()\n",
    "    \n",
    "    y_train = X_train.target # We set the training target equal to the target in the training set. This has to be done every iteration (as the fold and the data changes).\n",
    "    y_valid = X_valid.target # We set the validation target equal to the target in the validation set. This has to be done every iteration (as the fold and the data changes).\n",
    "    \n",
    "    X_train = X_train[useful_features] # We set our training data to be the previously defined useful features of X_train.\n",
    "    X_valid = X_valid[useful_features] # We set our validation data to be the previously defined useful features of X_valid.\n",
    "    \n",
    "    # We activate the pipeline, which preprocesses the training data and fits the model (will take about 10 minutes to run)\n",
    "    my_pipeline_2.fit(X_train, y_train)\n",
    "\n",
    "    preds_valid = my_pipeline_2.predict(X_valid) # We instruct the pipeline to make predictions on X_valid.\n",
    "    preds_test = my_pipeline_2.predict(X_test) # We instruct the pipeline to make predictions on X_test.\n",
    "    \n",
    "    final_test_predictions_2.append(preds_test) # We append each of the test predictions on to our final_predictions list.\n",
    "    final_valid_predictions_2.update(dict(zip(valid_ids, preds_valid)))\n",
    "    \n",
    "    final_test_predictions_2.append(preds_test) # We append each of the test predictions on to our final_predictions list.\n",
    "    rmse = mean_squared_error(y_valid, preds_valid, squared=False) # We store the mean non squared error in a variable.\n",
    "    print(fold, rmse) # Print the fold number, and the mean non squared error for each fold.\n",
    "    scores_2.append(rmse) # We append the rmse value to the scores list.\n",
    "    \n",
    "print(np.mean(scores_2), np.std(scores_2)) # Print the mean non square error average, and its standard deviation\n",
    "\n",
    "final_valid_predictions_2 = pd.DataFrame.from_dict(final_valid_predictions_2, orient=\"index\").reset_index()\n",
    "final_valid_predictions_2.columns = [\"id\", \"pred_2\"]\n",
    "final_valid_predictions_2.to_csv(\"train_pred_2.csv\", index=False)\n",
    "\n",
    "sample_submission.target = np.mean(np.column_stack(final_test_predictions_2), axis=1)\n",
    "sample_submission.columns = [\"id\", \"pred_2\"]\n",
    "sample_submission.to_csv(\"test_pred_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c2e99",
   "metadata": {
    "papermill": {
     "duration": 0.020926,
     "end_time": "2021-08-31T21:23:37.960590",
     "exception": false,
     "start_time": "2021-08-31T21:23:37.939664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model Blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dfd3b1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T21:23:38.023562Z",
     "iopub.status.busy": "2021-08-31T21:23:38.022732Z",
     "iopub.status.idle": "2021-08-31T21:23:41.042441Z",
     "shell.execute_reply": "2021-08-31T21:23:41.041785Z",
     "shell.execute_reply.started": "2021-08-31T20:29:44.764794Z"
    },
    "papermill": {
     "duration": 3.047669,
     "end_time": "2021-08-31T21:23:41.042632",
     "exception": false,
     "start_time": "2021-08-31T21:23:37.994963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>...</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>target</th>\n",
       "      <th>kfold</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>pred_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389470</td>\n",
       "      <td>0.267559</td>\n",
       "      <td>0.237281</td>\n",
       "      <td>0.377873</td>\n",
       "      <td>0.322401</td>\n",
       "      <td>0.869850</td>\n",
       "      <td>8.113634</td>\n",
       "      <td>9</td>\n",
       "      <td>8.527834</td>\n",
       "      <td>8.538169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594928</td>\n",
       "      <td>0.341439</td>\n",
       "      <td>0.906013</td>\n",
       "      <td>0.921701</td>\n",
       "      <td>0.261975</td>\n",
       "      <td>0.465083</td>\n",
       "      <td>8.481233</td>\n",
       "      <td>1</td>\n",
       "      <td>8.331272</td>\n",
       "      <td>8.350676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555205</td>\n",
       "      <td>0.843531</td>\n",
       "      <td>0.748809</td>\n",
       "      <td>0.620126</td>\n",
       "      <td>0.541474</td>\n",
       "      <td>0.763846</td>\n",
       "      <td>8.364351</td>\n",
       "      <td>8</td>\n",
       "      <td>8.222940</td>\n",
       "      <td>8.206738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.679618</td>\n",
       "      <td>0.574844</td>\n",
       "      <td>0.346010</td>\n",
       "      <td>0.714610</td>\n",
       "      <td>0.540150</td>\n",
       "      <td>0.280682</td>\n",
       "      <td>8.049253</td>\n",
       "      <td>2</td>\n",
       "      <td>8.400646</td>\n",
       "      <td>8.461390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684501</td>\n",
       "      <td>0.956692</td>\n",
       "      <td>1.000773</td>\n",
       "      <td>0.776742</td>\n",
       "      <td>0.625849</td>\n",
       "      <td>0.250823</td>\n",
       "      <td>7.972260</td>\n",
       "      <td>1</td>\n",
       "      <td>8.209665</td>\n",
       "      <td>8.299101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8  ...     cont8     cont9  \\\n",
       "0   1    B    B    B    C    B    B    A    E    C  ...  0.389470  0.267559   \n",
       "1   2    B    B    A    A    B    D    A    F    A  ...  0.594928  0.341439   \n",
       "2   3    A    A    A    C    B    D    A    D    A  ...  0.555205  0.843531   \n",
       "3   4    B    B    A    C    B    D    A    E    C  ...  0.679618  0.574844   \n",
       "4   6    A    A    A    C    B    D    A    E    A  ...  0.684501  0.956692   \n",
       "\n",
       "     cont10    cont11    cont12    cont13    target  kfold    pred_1    pred_2  \n",
       "0  0.237281  0.377873  0.322401  0.869850  8.113634      9  8.527834  8.538169  \n",
       "1  0.906013  0.921701  0.261975  0.465083  8.481233      1  8.331272  8.350676  \n",
       "2  0.748809  0.620126  0.541474  0.763846  8.364351      8  8.222940  8.206738  \n",
       "3  0.346010  0.714610  0.540150  0.280682  8.049253      2  8.400646  8.461390  \n",
       "4  1.000773  0.776742  0.625849  0.250823  7.972260      1  8.209665  8.299101  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full = pd.read_csv(\"../input/train-folds-k-folds-30-days-of-ml/train_folds_\"+ str(fold_num)+ \"_folds.csv\")\n",
    "X_test_full = pd.read_csv(\"../input/30-days-of-ml/test.csv\")\n",
    "sample_submission = pd.read_csv(\"../input/30-days-of-ml/sample_submission.csv\")\n",
    "\n",
    "df1 = pd.read_csv(\"train_pred_1.csv\")\n",
    "df2 = pd.read_csv(\"train_pred_2.csv\")\n",
    "\n",
    "df_test1 = pd.read_csv(\"test_pred_1.csv\")\n",
    "df_test2 = pd.read_csv(\"test_pred_2.csv\")\n",
    "\n",
    "X_full = X_full.merge(df1, on=\"id\", how=\"left\")\n",
    "X_full = X_full.merge(df2, on=\"id\", how=\"left\")\n",
    "\n",
    "X_test_full = X_test_full.merge(df_test1, on=\"id\", how=\"left\")\n",
    "X_test_full = X_test_full.merge(df_test2, on=\"id\", how=\"left\")\n",
    "\n",
    "X_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cfbb313",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T21:23:41.096899Z",
     "iopub.status.busy": "2021-08-31T21:23:41.094849Z",
     "iopub.status.idle": "2021-08-31T21:23:42.675303Z",
     "shell.execute_reply": "2021-08-31T21:23:42.676949Z",
     "shell.execute_reply.started": "2021-08-31T20:35:43.607758Z"
    },
    "papermill": {
     "duration": 1.612822,
     "end_time": "2021-08-31T21:23:42.677264",
     "exception": false,
     "start_time": "2021-08-31T21:23:41.064442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7151671615781626\n",
      "1 0.716550659102781\n",
      "2 0.7203973853382206\n",
      "3 0.7270745422373982\n",
      "4 0.7199691399633833\n",
      "5 0.7181413316688922\n",
      "6 0.7168325916818599\n",
      "7 0.7205987961221332\n",
      "8 0.7158858086237615\n",
      "9 0.7117439291860375\n",
      "0.718236134550263 0.003915163318265625\n"
     ]
    }
   ],
   "source": [
    "useful_features = [\"pred_1\", \"pred_2\"]\n",
    "X_test_full = X_test_full[useful_features]\n",
    "\n",
    "final_predictions = []\n",
    "scores = []\n",
    "for fold in range(fold_num):\n",
    "    X_train = X_full[X_full.kfold != fold].reset_index(drop=True) # We set the training data to be all folds different from the current fold number in the loop. We also reset the indices.\n",
    "    X_valid = X_full[X_full.kfold == fold].reset_index(drop=True) # The validation data is the current fold number in the loop. We also reset the indices.\n",
    "    X_test = X_test_full.copy() # We copy the original X_test to not alter or overwrite over it.\n",
    "    \n",
    "    y_train = X_train.target # We set the training target equal to the target in the training set. This has to be done every iteration (as the fold and the data changes).\n",
    "    y_valid = X_valid.target # We set the validation target equal to the target in the validation set. This has to be done every iteration (as the fold and the data changes).\n",
    "    \n",
    "    X_train = X_train[useful_features] # We set our training data to be the previously defined useful features of X_train.\n",
    "    X_valid = X_valid[useful_features] # We set our validation data to be the previously defined useful features of X_valid.\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    preds_valid = model.predict(X_valid)\n",
    "    test_preds = model.predict(X_test)\n",
    "    final_predictions.append(test_preds)\n",
    "    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n",
    "    print(fold, rmse)\n",
    "    scores.append(rmse)\n",
    "\n",
    "print(np.mean(scores), np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4bac667",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-31T21:23:42.780003Z",
     "iopub.status.busy": "2021-08-31T21:23:42.778500Z",
     "iopub.status.idle": "2021-08-31T21:23:43.453918Z",
     "shell.execute_reply": "2021-08-31T21:23:43.453208Z",
     "shell.execute_reply.started": "2021-08-31T20:35:51.94731Z"
    },
    "papermill": {
     "duration": 0.73161,
     "end_time": "2021-08-31T21:23:43.454073",
     "exception": false,
     "start_time": "2021-08-31T21:23:42.722463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_submission.target = np.mean(np.column_stack(final_predictions), axis=1)\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a1ccc2",
   "metadata": {
    "papermill": {
     "duration": 0.022289,
     "end_time": "2021-08-31T21:23:43.499478",
     "exception": false,
     "start_time": "2021-08-31T21:23:43.477189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1593.881195,
   "end_time": "2021-08-31T21:23:44.241010",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-31T20:57:10.359815",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
