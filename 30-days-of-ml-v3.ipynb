{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b81d1ee",
   "metadata": {
    "papermill": {
     "duration": 0.013099,
     "end_time": "2021-08-23T18:57:09.253860",
     "exception": false,
     "start_time": "2021-08-23T18:57:09.240761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 30 Days of ML v3, by Juan Torres\n",
    "### Based of my previous notebook, 30 Days of ML v2: https://www.kaggle.com/jtorres96/30-days-of-ml-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477d68c8",
   "metadata": {
    "papermill": {
     "duration": 0.011814,
     "end_time": "2021-08-23T18:57:09.279834",
     "exception": false,
     "start_time": "2021-08-23T18:57:09.268020",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook will be an iteration on the previous notebook. On this one, we will try using one-hot encoding with the previously built pipeline to see if performance is improved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99877f70",
   "metadata": {
    "papermill": {
     "duration": 0.012347,
     "end_time": "2021-08-23T18:57:09.305020",
     "exception": false,
     "start_time": "2021-08-23T18:57:09.292673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Importing libraries\n",
    "First, let's import the libraries we'll use for this first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ea069c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-08-23T18:57:09.343426Z",
     "iopub.status.busy": "2021-08-23T18:57:09.342356Z",
     "iopub.status.idle": "2021-08-23T18:57:10.654730Z",
     "shell.execute_reply": "2021-08-23T18:57:10.653898Z",
     "shell.execute_reply.started": "2021-08-23T18:41:20.331969Z"
    },
    "papermill": {
     "duration": 1.337762,
     "end_time": "2021-08-23T18:57:10.654901",
     "exception": false,
     "start_time": "2021-08-23T18:57:09.317139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/30-days-of-ml/sample_submission.csv\n",
      "/kaggle/input/30-days-of-ml/train.csv\n",
      "/kaggle/input/30-days-of-ml/test.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# For encoding categorical variables, splitting data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For the construction of the pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# For training random forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c481f9fe",
   "metadata": {
    "papermill": {
     "duration": 0.012037,
     "end_time": "2021-08-23T18:57:10.680068",
     "exception": false,
     "start_time": "2021-08-23T18:57:10.668031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Loading and preparing data\n",
    "Next, let's load the training and test data, separate our target from the training features, separate categorical and numerical columns and do a train_test_split to break off a validation set from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e38fbd6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T18:57:10.714990Z",
     "iopub.status.busy": "2021-08-23T18:57:10.714206Z",
     "iopub.status.idle": "2021-08-23T18:57:16.153614Z",
     "shell.execute_reply": "2021-08-23T18:57:16.153074Z",
     "shell.execute_reply.started": "2021-08-23T18:41:21.595075Z"
    },
    "papermill": {
     "duration": 5.461329,
     "end_time": "2021-08-23T18:57:16.153764",
     "exception": false,
     "start_time": "2021-08-23T18:57:10.692435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cont0</th>\n",
       "      <th>...</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>412957</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>G</td>\n",
       "      <td>1.035400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284988</td>\n",
       "      <td>0.777432</td>\n",
       "      <td>0.391158</td>\n",
       "      <td>0.575325</td>\n",
       "      <td>0.449008</td>\n",
       "      <td>0.808493</td>\n",
       "      <td>0.638874</td>\n",
       "      <td>0.620083</td>\n",
       "      <td>0.860327</td>\n",
       "      <td>0.796654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282449</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>0.552942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979303</td>\n",
       "      <td>0.388542</td>\n",
       "      <td>0.392169</td>\n",
       "      <td>0.251943</td>\n",
       "      <td>0.257683</td>\n",
       "      <td>0.289205</td>\n",
       "      <td>0.753284</td>\n",
       "      <td>0.108515</td>\n",
       "      <td>0.223745</td>\n",
       "      <td>0.867225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164867</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>F</td>\n",
       "      <td>0.300135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528495</td>\n",
       "      <td>0.877585</td>\n",
       "      <td>0.692778</td>\n",
       "      <td>0.394110</td>\n",
       "      <td>0.372842</td>\n",
       "      <td>0.603872</td>\n",
       "      <td>0.729337</td>\n",
       "      <td>0.572204</td>\n",
       "      <td>0.381617</td>\n",
       "      <td>0.528454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>0.423174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795973</td>\n",
       "      <td>0.254421</td>\n",
       "      <td>0.279979</td>\n",
       "      <td>0.244269</td>\n",
       "      <td>0.549558</td>\n",
       "      <td>0.318729</td>\n",
       "      <td>0.093974</td>\n",
       "      <td>0.428488</td>\n",
       "      <td>0.176486</td>\n",
       "      <td>0.250280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80790</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>A</td>\n",
       "      <td>0.667090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809695</td>\n",
       "      <td>0.412479</td>\n",
       "      <td>0.345747</td>\n",
       "      <td>0.537579</td>\n",
       "      <td>1.023082</td>\n",
       "      <td>0.433388</td>\n",
       "      <td>0.814753</td>\n",
       "      <td>0.655539</td>\n",
       "      <td>0.923203</td>\n",
       "      <td>0.467355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8     cont0  ...     cont4  \\\n",
       "id                                                             ...             \n",
       "412957    A    A    A    C    B    C    A    D    G  1.035400  ...  0.284988   \n",
       "282449    A    A    A    C    B    B    A    E    E  0.552942  ...  0.979303   \n",
       "164867    A    A    A    A    B    B    A    E    F  0.300135  ...  0.528495   \n",
       "541       A    B    A    C    B    B    A    E    E  0.423174  ...  0.795973   \n",
       "80790     B    B    A    C    B    D    A    E    A  0.667090  ...  0.809695   \n",
       "\n",
       "           cont5     cont6     cont7     cont8     cont9    cont10    cont11  \\\n",
       "id                                                                             \n",
       "412957  0.777432  0.391158  0.575325  0.449008  0.808493  0.638874  0.620083   \n",
       "282449  0.388542  0.392169  0.251943  0.257683  0.289205  0.753284  0.108515   \n",
       "164867  0.877585  0.692778  0.394110  0.372842  0.603872  0.729337  0.572204   \n",
       "541     0.254421  0.279979  0.244269  0.549558  0.318729  0.093974  0.428488   \n",
       "80790   0.412479  0.345747  0.537579  1.023082  0.433388  0.814753  0.655539   \n",
       "\n",
       "          cont12    cont13  \n",
       "id                          \n",
       "412957  0.860327  0.796654  \n",
       "282449  0.223745  0.867225  \n",
       "164867  0.381617  0.528454  \n",
       "541     0.176486  0.250280  \n",
       "80790   0.923203  0.467355  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training and test data. We set index_col=0 in the code cell below to use the id column to index the DataFrame. \n",
    "X_full = pd.read_csv(\"../input/30-days-of-ml/train.csv\", index_col=0)\n",
    "X_test_full = pd.read_csv(\"../input/30-days-of-ml/test.csv\", index_col=0)\n",
    "\n",
    "# Separate target (designated \"y\") from the training features.\n",
    "y = X_full['target']\n",
    "X = X_full.drop(['target'], axis=1)\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Select categorical columns with relatively low cardinality\n",
    "object_cols = [col for col in X.columns if 'cat' in col and X[col].nunique() < 10] # We are separating the categorical columns by the column name, previously we had done this by checking the data type of each column.\n",
    "\n",
    "# Select numerical columns\n",
    "num_cols = [col for col in X.columns if 'cont' in col]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = object_cols + num_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "X_test = X_test_full[my_cols].copy()\n",
    "\n",
    "# Take a peek at the training data\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b08bae7",
   "metadata": {
    "papermill": {
     "duration": 0.012773,
     "end_time": "2021-08-23T18:57:16.180033",
     "exception": false,
     "start_time": "2021-08-23T18:57:16.167260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Building the pipeline\n",
    "### 3.1: Define Preprocessing Steps\n",
    "We'll use the ColumnTransformer class to bundle together different preprocessing steps. The code below will impute missing values in numerical and categorical data, and apply a **one-hot encoding** to categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b424174a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T18:57:16.212522Z",
     "iopub.status.busy": "2021-08-23T18:57:16.211835Z",
     "iopub.status.idle": "2021-08-23T18:57:16.214926Z",
     "shell.execute_reply": "2021-08-23T18:57:16.214317Z",
     "shell.execute_reply.started": "2021-08-23T18:41:25.782489Z"
    },
    "papermill": {
     "duration": 0.02187,
     "end_time": "2021-08-23T18:57:16.215073",
     "exception": false,
     "start_time": "2021-08-23T18:57:16.193203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, num_cols),('cat', categorical_transformer, object_cols)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0878dd58",
   "metadata": {
    "papermill": {
     "duration": 0.012738,
     "end_time": "2021-08-23T18:57:16.241110",
     "exception": false,
     "start_time": "2021-08-23T18:57:16.228372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.2: Define the Model\n",
    "Next, we define a random forest model with the familiar RandomForestRegressor class, with the same parameters as the v1 notebook to ensure repeatability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "914c0dc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T18:57:16.272210Z",
     "iopub.status.busy": "2021-08-23T18:57:16.271543Z",
     "iopub.status.idle": "2021-08-23T18:57:16.274248Z",
     "shell.execute_reply": "2021-08-23T18:57:16.273632Z",
     "shell.execute_reply.started": "2021-08-23T18:41:25.788837Z"
    },
    "papermill": {
     "duration": 0.020114,
     "end_time": "2021-08-23T18:57:16.274383",
     "exception": false,
     "start_time": "2021-08-23T18:57:16.254269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the model \n",
    "model = RandomForestRegressor(random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7a9a79",
   "metadata": {
    "papermill": {
     "duration": 0.012735,
     "end_time": "2021-08-23T18:57:16.300504",
     "exception": false,
     "start_time": "2021-08-23T18:57:16.287769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.3: Create and Evaluate the Pipeline\n",
    "Finally, we use the Pipeline class to define a pipeline that bundles the preprocessing and modeling steps. Let's remember two important things:\n",
    "* With the pipeline, we preprocess the training data and fit the model in a single line of code. (In contrast, without a pipeline, we have to do imputation, one-hot encoding, and model training in separate steps. This becomes especially messy if we have to deal with both numerical and categorical variables!)\n",
    "* With the pipeline, we supply the unprocessed features in X_valid to the predict() command, and the pipeline automatically preprocesses the features before generating predictions. (However, without a pipeline, we have to remember to preprocess the validation data before making predictions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28eaa09f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T18:57:16.342463Z",
     "iopub.status.busy": "2021-08-23T18:57:16.341713Z",
     "iopub.status.idle": "2021-08-23T19:09:23.237204Z",
     "shell.execute_reply": "2021-08-23T19:09:23.237812Z",
     "shell.execute_reply.started": "2021-08-23T18:41:50.546608Z"
    },
    "papermill": {
     "duration": 726.924384,
     "end_time": "2021-08-23T19:09:23.238150",
     "exception": false,
     "start_time": "2021-08-23T18:57:16.313766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7371147047329659\n"
     ]
    }
   ],
   "source": [
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
    "\n",
    "# Preprocessing of training data, fit model (will take about 10 minutes to run)\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds_valid = my_pipeline.predict(X_valid)\n",
    "\n",
    "print(mean_squared_error(y_valid, preds_valid, squared=False)) # We set squared=False to get the root mean squared error (RMSE) on the validation data, as this is the competition's requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d4fe99",
   "metadata": {
    "papermill": {
     "duration": 0.012974,
     "end_time": "2021-08-23T19:09:23.264361",
     "exception": false,
     "start_time": "2021-08-23T19:09:23.251387",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Previous score was 0.7375392165180452, let's compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0a338d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T19:09:23.295633Z",
     "iopub.status.busy": "2021-08-23T19:09:23.294625Z",
     "iopub.status.idle": "2021-08-23T19:09:23.303053Z",
     "shell.execute_reply": "2021-08-23T19:09:23.303653Z",
     "shell.execute_reply.started": "2021-08-23T18:52:39.297882Z"
    },
    "papermill": {
     "duration": 0.026054,
     "end_time": "2021-08-23T19:09:23.303833",
     "exception": false,
     "start_time": "2021-08-23T19:09:23.277779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00042451178507929566\n"
     ]
    }
   ],
   "source": [
    "print(abs(0.7375392165180452-mean_squared_error(y_valid, preds_valid, squared=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40146b96",
   "metadata": {
    "papermill": {
     "duration": 0.0139,
     "end_time": "2021-08-23T19:09:23.331818",
     "exception": false,
     "start_time": "2021-08-23T19:09:23.317918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This is a very small difference accountable for the fact that we are comparing the X_valid predictions to the X_test predictions, so we can consider that the pipeline was correctly implemented! Now, let's obtain the predictions using the X_test set, and compare that MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452ebdc0",
   "metadata": {
    "papermill": {
     "duration": 0.013371,
     "end_time": "2021-08-23T19:09:23.359048",
     "exception": false,
     "start_time": "2021-08-23T19:09:23.345677",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Submit predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38e8d794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-23T19:09:23.402963Z",
     "iopub.status.busy": "2021-08-23T19:09:23.402260Z",
     "iopub.status.idle": "2021-08-23T19:09:39.632027Z",
     "shell.execute_reply": "2021-08-23T19:09:39.631453Z",
     "shell.execute_reply.started": "2021-08-23T18:55:33.159381Z"
    },
    "papermill": {
     "duration": 16.25926,
     "end_time": "2021-08-23T19:09:39.632188",
     "exception": false,
     "start_time": "2021-08-23T19:09:23.372928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the model to generate predictions\n",
    "predictions = my_pipeline.predict(X_test)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "output = pd.DataFrame({'Id': X_test.index,\n",
    "                       'target': predictions})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb3d151",
   "metadata": {
    "papermill": {
     "duration": 0.013759,
     "end_time": "2021-08-23T19:09:39.660026",
     "exception": false,
     "start_time": "2021-08-23T19:09:39.646267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next up, let's try using an XGBoost model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 759.738478,
   "end_time": "2021-08-23T19:09:40.586442",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-23T18:57:00.847964",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
