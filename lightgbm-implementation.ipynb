{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2624a7b6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.010126,
     "end_time": "2021-08-25T17:48:26.702801",
     "exception": false,
     "start_time": "2021-08-25T17:48:26.692675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LightGBM Implementation for 30 Days of ML Challenge, by Juan Torres\n",
    "\n",
    "#### Based on Svyatoslav Sokolov's notebook:\n",
    "\n",
    "https://www.kaggle.com/svyatoslavsokolov/tps-feb-2021-lgbm-simple-version?select=train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd02c0",
   "metadata": {
    "papermill": {
     "duration": 0.008479,
     "end_time": "2021-08-25T17:48:26.720460",
     "exception": false,
     "start_time": "2021-08-25T17:48:26.711981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A quick word about our previous notebook. We had concluded that Log Transformation was the way to go, but unfortunately the model performed horribly with the test data (obtaining scores of 0.77!). On a second try, the predictions were formulated with stardardization for numeric data and thankfully the model performed much better (obtaining scores of 0.72), so we're going to go with normalization. \n",
    "\n",
    "For this notebook, we will try switching regressors. XGBoost has served us well up until now without any parameter optimization, but available literature on the internet shows that better results can be achieved with LightGBM. We will incorporate this into our current notebook and see if the results are improved. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caf47fa",
   "metadata": {
    "papermill": {
     "duration": 0.00889,
     "end_time": "2021-08-25T17:48:26.738308",
     "exception": false,
     "start_time": "2021-08-25T17:48:26.729418",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c09a69ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T17:48:26.770800Z",
     "iopub.status.busy": "2021-08-25T17:48:26.770101Z",
     "iopub.status.idle": "2021-08-25T17:48:30.112543Z",
     "shell.execute_reply": "2021-08-25T17:48:30.111521Z",
     "shell.execute_reply.started": "2021-08-25T17:17:13.359289Z"
    },
    "papermill": {
     "duration": 3.365501,
     "end_time": "2021-08-25T17:48:30.112737",
     "exception": false,
     "start_time": "2021-08-25T17:48:26.747236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/30-days-of-ml/sample_submission.csv\n",
      "/kaggle/input/30-days-of-ml/train.csv\n",
      "/kaggle/input/30-days-of-ml/test.csv\n",
      "/kaggle/input/train-folds-30-days-of-ml/train_folds.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# For one-hot encoding categorical variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# For the construction of the pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# For training the LGBM regressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# For the mean squared error needed to calculate our scores\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af3521",
   "metadata": {
    "papermill": {
     "duration": 0.00927,
     "end_time": "2021-08-25T17:48:30.132023",
     "exception": false,
     "start_time": "2021-08-25T17:48:30.122753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Loading and preparing data and pipeline construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c844f52a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T17:48:30.155451Z",
     "iopub.status.busy": "2021-08-25T17:48:30.154900Z",
     "iopub.status.idle": "2021-08-25T17:48:33.475560Z",
     "shell.execute_reply": "2021-08-25T17:48:33.474468Z",
     "shell.execute_reply.started": "2021-08-25T17:17:18.164155Z"
    },
    "papermill": {
     "duration": 3.334329,
     "end_time": "2021-08-25T17:48:33.475717",
     "exception": false,
     "start_time": "2021-08-25T17:48:30.141388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the training and test data. \n",
    "X_full = pd.read_csv(\"../input/train-folds-30-days-of-ml/train_folds.csv\")\n",
    "X_test_full = pd.read_csv(\"../input/30-days-of-ml/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58525600",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T17:48:33.507039Z",
     "iopub.status.busy": "2021-08-25T17:48:33.503064Z",
     "iopub.status.idle": "2021-08-25T17:48:33.527372Z",
     "shell.execute_reply": "2021-08-25T17:48:33.526889Z",
     "shell.execute_reply.started": "2021-08-25T17:17:21.804294Z"
    },
    "papermill": {
     "duration": 0.04246,
     "end_time": "2021-08-25T17:48:33.527510",
     "exception": false,
     "start_time": "2021-08-25T17:48:33.485050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We select all features except \"id\", \"target\" and \"kfold\", as these are not predictors of our target.\n",
    "useful_features = [c for c in X_full.columns if c not in (\"id\", \"target\", \"kfold\")]\n",
    "\n",
    "# Select numerical columns\n",
    "num_cols = [col for col in useful_features if 'cont' in col]\n",
    "\n",
    "# We select categorical columns. Note that we dropped the cardinality check.\n",
    "object_cols = [col for col in useful_features if 'cat' in col]\n",
    "\n",
    "# We build X_test out of X_test_full, but only selecting the useful features.\n",
    "X_test = X_test_full[useful_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2e0ea04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T17:48:33.551958Z",
     "iopub.status.busy": "2021-08-25T17:48:33.551332Z",
     "iopub.status.idle": "2021-08-25T17:48:33.555362Z",
     "shell.execute_reply": "2021-08-25T17:48:33.554941Z",
     "shell.execute_reply.started": "2021-08-25T17:17:21.836202Z"
    },
    "papermill": {
     "duration": 0.018114,
     "end_time": "2021-08-25T17:48:33.555471",
     "exception": false,
     "start_time": "2021-08-25T17:48:33.537357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data, we use a StandardScaler to apply standardization.\n",
    "numerical_transformer = preprocessing.StandardScaler()\n",
    "\n",
    "# Preprocessing for categorical data and one-hot encoding.\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, num_cols),('cat', categorical_transformer, object_cols)])\n",
    "\n",
    "# Define the model \n",
    "model = LGBMRegressor(device=\"gpu\") # We set this to be a LGBMRegressor, with the only parameter for the time specifying training on a GPU.\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ccb206",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T17:48:33.581416Z",
     "iopub.status.busy": "2021-08-25T17:48:33.580576Z",
     "iopub.status.idle": "2021-08-25T17:49:10.749874Z",
     "shell.execute_reply": "2021-08-25T17:49:10.750689Z",
     "shell.execute_reply.started": "2021-08-25T17:17:27.260210Z"
    },
    "papermill": {
     "duration": 37.186528,
     "end_time": "2021-08-25T17:49:10.750866",
     "exception": false,
     "start_time": "2021-08-25T17:48:33.564338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7223753982194191\n",
      "1 0.7301754794491622\n",
      "2 0.7255235407438366\n",
      "3 0.7244993511962095\n",
      "4 0.7203523408978102\n",
      "0.7245852221012876 0.0033147970608888955\n"
     ]
    }
   ],
   "source": [
    "# We set up a list to store the final predictions.\n",
    "final_predictions = []\n",
    "\n",
    "# We set up a list for storing the mean non squared error scores.\n",
    "scores = []\n",
    "\n",
    "# We set the loop to loop across all of the folds. Since we have 5 folds, the loop range will be range(5).\n",
    "for fold in range(5):\n",
    "    X_train = X_full[X_full.kfold != fold].reset_index(drop=True) # We set the training data to be all folds different from the current fold number in the loop. We also reset the indices.\n",
    "    X_valid = X_full[X_full.kfold == fold].reset_index(drop=True) # The validation data is the current fold number in the loop. We also reset the indices.\n",
    "    X_test_copy = X_test.copy() # We copy the original X_test to not alter or overwrite over it.\n",
    "    \n",
    "    y_train = X_train.target # We set the training target equal to the target in the training set. This has to be done every iteration (as the fold and the data changes).\n",
    "    y_valid = X_valid.target # We set the validation target equal to the target in the validation set. This has to be done every iteration (as the fold and the data changes).\n",
    "    \n",
    "    X_train = X_train[useful_features] # We set our training data to be the previously defined useful features of X_train.\n",
    "    X_valid = X_valid[useful_features] # We set our validation data to be the previously defined useful features of X_valid.\n",
    "    \n",
    "    # We activate the pipeline, which preprocesses the training data and fits the model (will take about 10 minutes to run)\n",
    "    my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    preds_valid = my_pipeline.predict(X_valid) # We instruct the pipeline to make predictions on X_valid.\n",
    "    preds_test = my_pipeline.predict(X_test) # We instruct the pipeline to make predictions on X_test.\n",
    "    final_predictions.append(preds_test) # We append each of the test predictions on to our final_predictions list.\n",
    "    rmse = mean_squared_error(y_valid, preds_valid, squared=False) # We store the mean non squared error in a variable.\n",
    "    print(fold, rmse) # Print the fold number, and the mean non squared error for each fold.\n",
    "    scores.append(rmse) # We append the rmse value to the scores list.\n",
    "    \n",
    "print(np.mean(scores), np.std(scores)) # Print the mean non square error average, and its standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d73a0c",
   "metadata": {
    "papermill": {
     "duration": 0.010229,
     "end_time": "2021-08-25T17:49:10.773034",
     "exception": false,
     "start_time": "2021-08-25T17:49:10.762805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Looking good on paper, let's stack these predictions and build the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b0fc122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T17:49:10.799351Z",
     "iopub.status.busy": "2021-08-25T17:49:10.798201Z",
     "iopub.status.idle": "2021-08-25T17:49:10.807505Z",
     "shell.execute_reply": "2021-08-25T17:49:10.807051Z",
     "shell.execute_reply.started": "2021-08-25T17:18:40.600646Z"
    },
    "papermill": {
     "duration": 0.024239,
     "end_time": "2021-08-25T17:49:10.807611",
     "exception": false,
     "start_time": "2021-08-25T17:49:10.783372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = np.mean(np.column_stack(final_predictions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faccbf8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-25T17:49:10.832718Z",
     "iopub.status.busy": "2021-08-25T17:49:10.832172Z",
     "iopub.status.idle": "2021-08-25T17:49:11.449336Z",
     "shell.execute_reply": "2021-08-25T17:49:11.448797Z",
     "shell.execute_reply.started": "2021-08-25T17:33:28.753941Z"
    },
    "papermill": {
     "duration": 0.631742,
     "end_time": "2021-08-25T17:49:11.449469",
     "exception": false,
     "start_time": "2021-08-25T17:49:10.817727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the predictions to a CSV file\n",
    "output = pd.DataFrame({'Id': X_test_full.id, 'target': predictions})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b680d178",
   "metadata": {
    "papermill": {
     "duration": 0.010001,
     "end_time": "2021-08-25T17:49:11.469818",
     "exception": false,
     "start_time": "2021-08-25T17:49:11.459817",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The pipeline implementation is very convenient for these types of changes. Next up, we will follow Abhishek's tutorials once again to continue improving our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 53.589203,
   "end_time": "2021-08-25T17:49:13.328870",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-25T17:48:19.739667",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
